#!/usr/bin/env python3
"""
KæŠ˜äº¤å‰éªŒè¯è„šæœ¬ - éªŒè¯æ¨¡å‹çš„çœŸå®æ€§èƒ½
"""

import os
import json
import pickle
import numpy as np
import torch
import torch.nn.functional as F
from torch_geometric.data import Data, DataLoader
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import glob
from collections import defaultdict
import argparse

def load_processed_data():
    """åŠ è½½æ‰€æœ‰å¤„ç†è¿‡çš„æ•°æ®"""
    data_dir = "data/processed"
    all_data = []
    all_labels = []
    file_names = []
    
    for file_path in glob.glob(os.path.join(data_dir, "*_processed.npz")):
        file_name = os.path.basename(file_path).replace("_processed.npz", "")
        try:
            data = np.load(file_path, allow_pickle=True)
            
            # è·å–æ•°æ®
            node_features = data['node_features']
            edge_index = data['edge_index']
            node_labels = data['node_classes']
            
            # è½¬æ¢ä¸ºPyGæ•°æ®æ ¼å¼
            graph_data = Data(
                x=torch.FloatTensor(node_features),
                edge_index=torch.LongTensor(edge_index),
                y=torch.LongTensor(node_labels)
            )
            
            all_data.append(graph_data)
            all_labels.extend(node_labels)
            file_names.append(file_name)
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            continue
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(all_data)} ä¸ªæ¡ˆä¾‹")
    return all_data, all_labels, file_names

def create_simple_model(input_dim, num_classes):
    """åˆ›å»ºç®€å•çš„åŸºçº¿æ¨¡å‹"""
    from torch_geometric.nn import GCNConv, global_mean_pool
    import torch.nn as nn
    
    class SimpleGCN(torch.nn.Module):
        def __init__(self, input_dim, hidden_dim, num_classes):
            super(SimpleGCN, self).__init__()
            self.conv1 = GCNConv(input_dim, hidden_dim)
            self.conv2 = GCNConv(hidden_dim, hidden_dim)
            self.classifier = nn.Linear(hidden_dim, num_classes)
            self.dropout = nn.Dropout(0.5)
            
        def forward(self, x, edge_index, batch=None):
            x = F.relu(self.conv1(x, edge_index))
            x = self.dropout(x)
            x = F.relu(self.conv2(x, edge_index))
            x = self.dropout(x)
            x = self.classifier(x)
            return x
    
    return SimpleGCN(input_dim, 64, num_classes)

def k_fold_validation(k=5):
    """æ‰§è¡ŒKæŠ˜äº¤å‰éªŒè¯"""
    print(f"ğŸ”„ å¼€å§‹{k}æŠ˜äº¤å‰éªŒè¯...")
    
    # åŠ è½½æ•°æ®
    all_data, all_labels, file_names = load_processed_data()
    
    if len(all_data) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®æ–‡ä»¶")
        return
    
    # åˆ›å»ºKæŠ˜åˆ†å‰²
    kfold = KFold(n_splits=k, shuffle=True, random_state=42)
    
    fold_results = []
    all_predictions = []
    all_true_labels = []
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    for fold, (train_idx, val_idx) in enumerate(kfold.split(all_data)):
        print(f"\nğŸ“Š è®­ç»ƒæŠ˜ {fold+1}/{k}")
        print(f"   è®­ç»ƒæ¡ˆä¾‹: {len(train_idx)}, éªŒè¯æ¡ˆä¾‹: {len(val_idx)}")
        
        # åˆ†å‰²æ•°æ®
        train_data = [all_data[i] for i in train_idx]
        val_data = [all_data[i] for i in val_idx]
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_data, batch_size=16, shuffle=True)
        val_loader = DataLoader(val_data, batch_size=16, shuffle=False)
        
        # åˆ›å»ºæ¨¡å‹
        input_dim = all_data[0].x.shape[1]
        num_classes = len(set(all_labels))
        model = create_simple_model(input_dim, num_classes).to(device)
        
        # è®­ç»ƒé…ç½®
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        criterion = torch.nn.CrossEntropyLoss()
        
        # è®­ç»ƒæ¨¡å‹
        model.train()
        for epoch in range(50):  # å‡å°‘è®­ç»ƒè½®æ•°
            total_loss = 0
            for batch in train_loader:
                batch = batch.to(device)
                optimizer.zero_grad()
                out = model(batch.x, batch.edge_index, batch.batch)
                loss = criterion(out, batch.y)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
            
            if (epoch + 1) % 10 == 0:
                print(f"   Epoch {epoch+1}: Loss = {total_loss/len(train_loader):.4f}")
        
        # éªŒè¯æ¨¡å‹
        model.eval()
        fold_predictions = []
        fold_true_labels = []
        
        with torch.no_grad():
            for batch in val_loader:
                batch = batch.to(device)
                out = model(batch.x, batch.edge_index, batch.batch)
                pred = out.argmax(dim=1)
                
                fold_predictions.extend(pred.cpu().numpy())
                fold_true_labels.extend(batch.y.cpu().numpy())
        
        # è®¡ç®—æŠ˜çš„æ€§èƒ½
        fold_accuracy = accuracy_score(fold_true_labels, fold_predictions)
        print(f"   æŠ˜ {fold+1} å‡†ç¡®ç‡: {fold_accuracy:.4f}")
        
        fold_results.append({
            'fold': fold + 1,
            'accuracy': fold_accuracy,
            'train_cases': [file_names[i] for i in train_idx],
            'val_cases': [file_names[i] for i in val_idx],
            'predictions': fold_predictions,
            'true_labels': fold_true_labels
        })
        
        all_predictions.extend(fold_predictions)
        all_true_labels.extend(fold_true_labels)
    
    # è®¡ç®—æ€»ä½“ç»“æœ
    overall_accuracy = accuracy_score(all_true_labels, all_predictions)
    fold_accuracies = [r['accuracy'] for r in fold_results]
    
    print(f"\nğŸ“Š KæŠ˜äº¤å‰éªŒè¯ç»“æœ:")
    print(f"   å¹³å‡å‡†ç¡®ç‡: {np.mean(fold_accuracies):.4f} Â± {np.std(fold_accuracies):.4f}")
    print(f"   æœ€é«˜å‡†ç¡®ç‡: {np.max(fold_accuracies):.4f}")
    print(f"   æœ€ä½å‡†ç¡®ç‡: {np.min(fold_accuracies):.4f}")
    print(f"   æ€»ä½“å‡†ç¡®ç‡: {overall_accuracy:.4f}")
    
    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    print(f"\nğŸ“‹ åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(all_true_labels, all_predictions))
    
    # ä¿å­˜ç»“æœ
    results = {
        'k_folds': k,
        'fold_results': fold_results,
        'summary': {
            'mean_accuracy': np.mean(fold_accuracies),
            'std_accuracy': np.std(fold_accuracies),
            'max_accuracy': np.max(fold_accuracies),
            'min_accuracy': np.min(fold_accuracies),
            'overall_accuracy': overall_accuracy
        },
        'classification_report': classification_report(all_true_labels, all_predictions, output_dict=True),
        'confusion_matrix': confusion_matrix(all_true_labels, all_predictions).tolist()
    }
    
    with open(f'k_fold_validation_results_{k}.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ° k_fold_validation_results_{k}.json")
    
    return results

def leave_one_out_validation():
    """ç•™ä¸€æ³•äº¤å‰éªŒè¯"""
    print("ğŸ”„ å¼€å§‹ç•™ä¸€æ³•äº¤å‰éªŒè¯...")
    
    all_data, all_labels, file_names = load_processed_data()
    
    if len(all_data) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®æ–‡ä»¶")
        return
    
    results = []
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    for i in range(len(all_data)):
        print(f"ğŸ“Š æµ‹è¯•æ¡ˆä¾‹ {i+1}/{len(all_data)}: {file_names[i]}")
        
        # åˆ†å‰²æ•°æ®
        test_data = [all_data[i]]
        train_data = [all_data[j] for j in range(len(all_data)) if j != i]
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_data, batch_size=16, shuffle=True)
        test_loader = DataLoader(test_data, batch_size=1, shuffle=False)
        
        # åˆ›å»ºæ¨¡å‹
        input_dim = all_data[0].x.shape[1]
        num_classes = len(set(all_labels))
        model = create_simple_model(input_dim, num_classes).to(device)
        
        # è®­ç»ƒé…ç½®
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        criterion = torch.nn.CrossEntropyLoss()
        
        # è®­ç»ƒæ¨¡å‹
        model.train()
        for epoch in range(30):  # å‡å°‘è®­ç»ƒè½®æ•°
            total_loss = 0
            for batch in train_loader:
                batch = batch.to(device)
                optimizer.zero_grad()
                out = model(batch.x, batch.edge_index, batch.batch)
                loss = criterion(out, batch.y)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
        
        # æµ‹è¯•æ¨¡å‹
        model.eval()
        test_predictions = []
        test_true_labels = []
        
        with torch.no_grad():
            for batch in test_loader:
                batch = batch.to(device)
                out = model(batch.x, batch.edge_index, batch.batch)
                pred = out.argmax(dim=1)
                
                test_predictions.extend(pred.cpu().numpy())
                test_true_labels.extend(batch.y.cpu().numpy())
        
        # è®¡ç®—å‡†ç¡®ç‡
        accuracy = accuracy_score(test_true_labels, test_predictions)
        print(f"   æ¡ˆä¾‹ {file_names[i]} å‡†ç¡®ç‡: {accuracy:.4f}")
        
        results.append({
            'case': file_names[i],
            'accuracy': accuracy,
            'predictions': test_predictions,
            'true_labels': test_true_labels
        })
    
    # è®¡ç®—æ€»ä½“ç»“æœ
    accuracies = [r['accuracy'] for r in results]
    mean_accuracy = np.mean(accuracies)
    
    print(f"\nğŸ“Š ç•™ä¸€æ³•äº¤å‰éªŒè¯ç»“æœ:")
    print(f"   å¹³å‡å‡†ç¡®ç‡: {mean_accuracy:.4f} Â± {np.std(accuracies):.4f}")
    print(f"   æœ€é«˜å‡†ç¡®ç‡: {np.max(accuracies):.4f}")
    print(f"   æœ€ä½å‡†ç¡®ç‡: {np.min(accuracies):.4f}")
    
    # ä¿å­˜ç»“æœ
    final_results = {
        'method': 'leave_one_out',
        'case_results': results,
        'summary': {
            'mean_accuracy': mean_accuracy,
            'std_accuracy': np.std(accuracies),
            'max_accuracy': np.max(accuracies),
            'min_accuracy': np.min(accuracies)
        }
    }
    
    with open('leave_one_out_validation_results.json', 'w') as f:
        json.dump(final_results, f, indent=2)
    
    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ° leave_one_out_validation_results.json")
    
    return final_results

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='KæŠ˜äº¤å‰éªŒè¯')
    parser.add_argument('--k', type=int, default=5, help='KæŠ˜æ•°é‡')
    parser.add_argument('--method', choices=['kfold', 'loo', 'both'], default='both', help='éªŒè¯æ–¹æ³•')
    
    args = parser.parse_args()
    
    if args.method in ['kfold', 'both']:
        k_fold_validation(args.k)
    
    if args.method in ['loo', 'both']:
        leave_one_out_validation()
