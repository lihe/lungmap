import torch
import torch.nn as nn
from models.modules.fp_module import TopoFPModule
from models.modules.sa_module import TopoSAModule
from models.modules.cnn3d import CNN3D

class CPRTaGNet(nn.Module):
    def __init__(self, num_classes=18, node_feature_dim=54, image_channels=1):
        super().__init__()
        self.num_classes = num_classes
        self.node_feature_dim = node_feature_dim
        
        # å›¾åƒæ¡ä»¶æå–è·¯å¾„
        self.image_encoder = nn.Sequential(
            CNN3D(in_channels=image_channels, hidden_channels=64),
            nn.AdaptiveAvgPool3d((1, 1, 1)),
            nn.Flatten()
        )
        # ç®€åŒ–å›¾åƒç‰¹å¾å¤„ç†ï¼Œå»æ‰BiLSTMï¼Œç›´æ¥ä½¿ç”¨CNN3Dè¾“å‡º
        self.image_proj = nn.Linear(64, 256)

        # SA å±‚ï¼ˆEncoderï¼‰
        self.sa1 = TopoSAModule(in_channels=node_feature_dim, out_channels=128, k=8, sample_ratio=0.5)
        self.sa2 = TopoSAModule(in_channels=128, out_channels=256, k=8, sample_ratio=0.5)

        # FP å±‚ï¼ˆDecoderï¼‰
        self.fp1 = TopoFPModule(in_channels=256, skip_channels=128, out_channels=128)
        self.fp2 = TopoFPModule(in_channels=128, skip_channels=node_feature_dim, out_channels=64)

        # åˆ†ç±»å™¨ï¼ˆç”¨äºåˆ†æ”¯æ ‡ç­¾é¢„æµ‹ï¼‰
        self.classifier = nn.Sequential(
            nn.Linear(64 + 256, 128),  # 64 from fp2_feat + 256 from image_cond
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes)
        )

    def forward(self, x_node, pos, edge_index, image_cubes):
        """
        x_node: èŠ‚ç‚¹ç‰¹å¾ [N, node_feature_dim]ï¼ˆå¦‚åæ ‡ + åŠå¾„ç­‰ï¼‰
        pos: èŠ‚ç‚¹ä½ç½® [N, 3]
        edge_index: å›¾è¿æ¥ [2, E]
        image_cubes: å¯¹åº”èŠ‚ç‚¹å›¾åƒå— [N, C, D, H, W]
        """
        try:
            batch_size = x_node.shape[0]
            
            # ===== å›¾åƒæ¡ä»¶è·¯å¾„ =====
            image_feat = self.image_encoder(image_cubes)  # [N, 64]
            image_cond = self.image_proj(image_feat)  # [N, 256]

            # ===== å›¾ç»“æ„è·¯å¾„ï¼šEncoderï¼ˆSAï¼‰ =====
            sa1_feat, sa1_pos, sa1_edge = self.sa1(x_node, pos, edge_index)  # [M1, 128]
            sa2_feat, sa2_pos, sa2_edge = self.sa2(sa1_feat, sa1_pos, sa1_edge)  # [M2, 256]

            # ===== å›¾ç»“æ„è·¯å¾„ï¼šDecoderï¼ˆFPï¼‰ =====
            fp1_feat = self.fp1(sa2_feat, sa2_pos, sa1_pos, sa1_feat)  # [M1, 128]
            fp2_feat = self.fp2(fp1_feat, sa1_pos, pos, x_node)        # [N, 64]

            # ===== ğŸ”§ ä¿®å¤ï¼šç¡®ä¿ç»´åº¦åŒ¹é… =====
            # æ£€æŸ¥fp2_featå’Œimage_condçš„ç»´åº¦æ˜¯å¦åŒ¹é…
            if fp2_feat.shape[0] != image_cond.shape[0]:
                print(f"âš ï¸  Dimension mismatch: fp2_feat {fp2_feat.shape}, image_cond {image_cond.shape}")
                # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨æ’å€¼è°ƒæ•´
                if fp2_feat.shape[0] < image_cond.shape[0]:
                    # ä»image_condä¸­é€‰æ‹©å¯¹åº”çš„ç‰¹å¾
                    image_cond = image_cond[:fp2_feat.shape[0]]
                else:
                    # å¯¹fp2_featè¿›è¡Œä¸‹é‡‡æ ·æˆ–é‡å¤
                    fp2_feat = fp2_feat[:image_cond.shape[0]]

            # ===== å¤šä»»åŠ¡è¾“å‡ºï¼ˆèåˆå›¾ç»“æ„ + å›¾åƒæ¡ä»¶ï¼‰ =====
            # ğŸ”§ ä¿®å¤ï¼šåŠ¨æ€å¤„ç†ç‰¹å¾ç»´åº¦ä¸åŒ¹é…
            expected_graph_dim = 64  # æœŸæœ›çš„å›¾ç‰¹å¾ç»´åº¦
            expected_image_dim = 256  # æœŸæœ›çš„å›¾åƒç‰¹å¾ç»´åº¦
            
            # è°ƒæ•´å›¾ç‰¹å¾ç»´åº¦
            if fp2_feat.shape[1] != expected_graph_dim:
                if not hasattr(self, '_graph_proj') or self._graph_proj.in_features != fp2_feat.shape[1]:
                    self._graph_proj = nn.Linear(fp2_feat.shape[1], expected_graph_dim).to(fp2_feat.device)
                fp2_feat = self._graph_proj(fp2_feat)
            
            # è°ƒæ•´å›¾åƒç‰¹å¾ç»´åº¦
            if image_cond.shape[1] != expected_image_dim:
                if not hasattr(self, '_image_proj_fix') or self._image_proj_fix.in_features != image_cond.shape[1]:
                    self._image_proj_fix = nn.Linear(image_cond.shape[1], expected_image_dim).to(image_cond.device)
                image_cond = self._image_proj_fix(image_cond)
            
            out_feat = torch.cat([fp2_feat, image_cond], dim=1)  # [N, 320]
            
            # ğŸ”§ ä¿®å¤ï¼šåŠ¨æ€å¤„ç†åˆ†ç±»å™¨è¾“å…¥ç»´åº¦
            expected_classifier_dim = 320  # 64 + 256
            if out_feat.shape[1] != expected_classifier_dim:
                if not hasattr(self, '_classifier_proj') or self._classifier_proj.in_features != out_feat.shape[1]:
                    self._classifier_proj = nn.Linear(out_feat.shape[1], expected_classifier_dim).to(out_feat.device)
                out_feat = self._classifier_proj(out_feat)
            
            logits = self.classifier(out_feat)  # [N, num_classes]

            return logits
            
        except Exception as e:
            print(f"âš ï¸  CPRTaGNet forward error: {e}")
            # é™çº§å¤„ç†ï¼šä»…ä½¿ç”¨å›¾åƒç‰¹å¾
            batch_size = x_node.shape[0]
            image_feat = self.image_encoder(image_cubes)  # [N, 64]
            image_cond = self.image_proj(image_feat)  # [N, 256]
            
            # åˆ›å»ºé›¶å›¾ç»“æ„ç‰¹å¾
            graph_feat = torch.zeros((batch_size, 64), device=x_node.device)
            out_feat = torch.cat([graph_feat, image_cond], dim=1)
            logits = self.classifier(out_feat)
            return logits