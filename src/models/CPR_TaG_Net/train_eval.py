
# === CPR-TaG-Net è®­ç»ƒä¸è¯„ä¼°ä»£ç  ===
import os
import torch
import yaml
from torch.utils.data import DataLoader
from models.cpr_tagnet import CPRTaGNet
from data.dataset import VesselDataset
import torch.nn as nn
import pickle
from tqdm import tqdm
from data.utils import bresenham_3d

def load_config(path):
    with open(path, 'r') as f:
        return yaml.safe_load(f)

# ========== 1. Train å‡½æ•° ==========
def train(model, dataloader, optimizer, criterion_label, criterion_seg=None, alpha=0.5):
    model.train()
    total_loss, correct, total = 0.0, 0, 0

    for batch in tqdm(dataloader):
        # ç”±äºbatch_size=1ï¼Œç›´æ¥è§£åŒ…
        x_node, pos, edge_index, image_cubes, labels, seg_mask = batch
        
        # å»æ‰batchç»´åº¦ï¼ˆå› ä¸ºbatch_size=1ï¼‰
        x_node = x_node.squeeze(0).cuda()
        pos = pos.squeeze(0).cuda() 
        edge_index = edge_index.squeeze(0).cuda()
        image_cubes = image_cubes.squeeze(0).cuda()
        labels = labels.squeeze(0).cuda()
        seg_mask = seg_mask.squeeze(0).cuda() if seg_mask is not None else None

        optimizer.zero_grad()
        outputs = model(x_node, pos, edge_index, image_cubes)

        if isinstance(outputs, tuple):
            logits, refined_seg = outputs
        else:
            logits = outputs
            refined_seg = None

        loss_label = criterion_label(logits, labels)
        if criterion_seg and refined_seg is not None:
            loss_seg = criterion_seg(refined_seg.squeeze(), seg_mask.float())
            loss = loss_label + alpha * loss_seg
        else:
            loss = loss_label

        loss.backward()
        optimizer.step()

        total_loss += loss.item() * x_node.size(0)
        pred = logits.argmax(dim=1)
        correct += (pred == labels).sum().item()
        total += labels.size(0)

    acc = correct / total
    avg_loss = total_loss / total
    return avg_loss, acc

# ========== 2. Validate å‡½æ•° ==========
def validate(model, dataloader, criterion_label):
    model.eval()
    total_loss, correct, total = 0.0, 0, 0
    all_preds, all_labels = [], []

    with torch.no_grad():
        for batch in tqdm(dataloader):
            # ç”±äºbatch_size=1ï¼Œç›´æ¥è§£åŒ…
            x_node, pos, edge_index, image_cubes, labels, _ = batch
            
            # å»æ‰batchç»´åº¦ï¼ˆå› ä¸ºbatch_size=1ï¼‰
            x_node = x_node.squeeze(0).cuda()
            pos = pos.squeeze(0).cuda() 
            edge_index = edge_index.squeeze(0).cuda()
            image_cubes = image_cubes.squeeze(0).cuda()
            labels = labels.squeeze(0).cuda()

            outputs = model(x_node, pos, edge_index, image_cubes)
            if isinstance(outputs, tuple):
                logits = outputs[0]
            else:
                logits = outputs

            loss = criterion_label(logits, labels)
            total_loss += loss.item() * x_node.size(0)

            pred = logits.argmax(dim=1)
            correct += (pred == labels).sum().item()
            total += labels.size(0)
            all_preds.append(pred.cpu())
            all_labels.append(labels.cpu())

    acc = correct / total
    avg_loss = total_loss / total
    return avg_loss, acc, torch.cat(all_preds), torch.cat(all_labels)

# ========== 3. Graph Completionï¼ˆæ¨ç†é˜¶æ®µï¼‰ ==========
def complete_graph(label_pred, node_pos, edge_index, label_rules):
    import networkx as nx
    G = nx.Graph()
    for i in range(edge_index.shape[1]):
        G.add_edge(edge_index[0, i].item(), edge_index[1, i].item())

    for lbl in torch.unique(label_pred):
        idx = torch.where(label_pred == lbl)[0].tolist()
        for i in idx:
            for j in idx:
                if i != j and not G.has_edge(i, j):
                    dist = torch.norm(node_pos[i] - node_pos[j])
                    if dist < 5.0:
                        G.add_edge(i, j)

    for (a, b) in label_rules:
        ai = torch.where(label_pred == a)[0].tolist()
        bi = torch.where(label_pred == b)[0].tolist()
        for i in ai:
            for j in bi:
                if not G.has_edge(i, j):
                    dist = torch.norm(node_pos[i] - node_pos[j])
                    if dist < 8.0:
                        G.add_edge(i, j)

    new_edges = torch.tensor(list(G.edges)).T.cuda()
    return new_edges

# ========== 4. åˆ†å‰²è¡¥å…¨ï¼ˆåŸºäº refined edgeï¼‰ ==========
def segmentation_completion(centerline_pos, refined_edge, volume_shape, radius=2):
    seg = torch.zeros(volume_shape, dtype=torch.uint8)
    for i, j in refined_edge.T:
        p1 = centerline_pos[i].cpu().numpy()
        p2 = centerline_pos[j].cpu().numpy()
        pts = bresenham_3d(p1, p2)
        for pt in pts:
            for dx in range(-radius, radius+1):
                for dy in range(-radius, radius+1):
                    for dz in range(-radius, radius+1):
                        x, y, z = pt[0]+dx, pt[1]+dy, pt[2]+dz
                        if 0 <= x < seg.shape[0] and 0 <= y < seg.shape[1] and 0 <= z < seg.shape[2]:
                            seg[x, y, z] = 1
    return seg

# ========== 5. å¯è§†åŒ–ä¸æ¶ˆèå®éªŒå¯¹ç…§ ==========
def plot_confusion_matrix(y_true, y_pred, class_names):
    import seaborn as sns
    import matplotlib.pyplot as plt
    from sklearn.metrics import confusion_matrix
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_names, yticklabels=class_names)
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.title("Confusion Matrix")
    plt.show()


def main():
    # ==== 1. åŠ è½½é…ç½® ====
    config = load_config('configs/train.yaml')
    os.makedirs(config['log']['save_dir'], exist_ok=True)

    # ==== 2. åŠ è½½æ•°æ® ====
    print("=== åŠ è½½æ•°æ® ===")
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(config['dataset']['train_data_path']):
        print(f"âŒ è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {config['dataset']['train_data_path']}")
        print("è¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬: python data/batch_preprocess.py")
        return
    
    if not os.path.exists(config['dataset']['val_data_path']):
        print(f"âŒ éªŒè¯æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {config['dataset']['val_data_path']}")
        print("è¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬: python data/batch_preprocess.py")
        return
    
    with open(config['dataset']['train_data_path'], 'rb') as f:
        train_data = pickle.load(f)
    with open(config['dataset']['val_data_path'], 'rb') as f:
        val_data = pickle.load(f)

    print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®:")
    print(f"   è®­ç»ƒé›†: {len(train_data)} ä¸ªç—…ä¾‹")
    print(f"   éªŒè¯é›†: {len(val_data)} ä¸ªç—…ä¾‹")
    
    # æ‰“å°æ•°æ®ç»Ÿè®¡ä¿¡æ¯
    total_train_nodes = sum(len(data['labels']) for data in train_data)
    total_val_nodes = sum(len(data['labels']) for data in val_data)
    
    print(f"   è®­ç»ƒé›†èŠ‚ç‚¹æ€»æ•°: {total_train_nodes}")
    print(f"   éªŒè¯é›†èŠ‚ç‚¹æ€»æ•°: {total_val_nodes}")
    
    # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
    all_labels = []
    for data in train_data:
        all_labels.extend(data['labels'].tolist())
    unique_labels = sorted(list(set(all_labels)))
    print(f"   æ ‡ç­¾ç±»åˆ«: {unique_labels} (å…±{len(unique_labels)}ç±»)")

    train_set = VesselDataset(train_data)
    val_set = VesselDataset(val_data)

    train_loader = DataLoader(train_set, batch_size=1, shuffle=True,
                              num_workers=config['dataset']['num_workers'])
    val_loader = DataLoader(val_set, batch_size=1, shuffle=False)

    # ==== 3. æ„å»ºæ¨¡å‹ ====
    print("\n=== æ„å»ºæ¨¡å‹ ===")
    
    # åŠ¨æ€ç¡®å®šç±»åˆ«æ•°
    num_classes = len(unique_labels) if len(unique_labels) <= 18 else 18
    print(f"æ¨¡å‹ç±»åˆ«æ•°: {num_classes}")
    
    model = CPRTaGNet(
        num_classes=num_classes,
        node_feature_dim=config['model']['node_feature_dim'],
        image_channels=config['model']['image_condition_channels']
    ).cuda()
    
    print(f"âœ… æ¨¡å‹æ„å»ºå®Œæˆ")
    
    # æ‰“å°æ¨¡å‹å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"   æ€»å‚æ•°æ•°: {total_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°æ•°: {trainable_params:,}")

    optimizer = torch.optim.Adam(model.parameters(),
                                 lr=config['training']['lr'],
                                 weight_decay=config['training']['weight_decay'])

    criterion_label = nn.CrossEntropyLoss()
    criterion_seg = nn.BCEWithLogitsLoss() if config['loss']['seg'] == "BCEWithLogits" else None

    # ==== 4. è®­ç»ƒå¾ªç¯ ====
    print(f"\n=== å¼€å§‹è®­ç»ƒ ===")
    print(f"è®­ç»ƒé…ç½®:")
    print(f"   Epochs: {config['training']['epochs']}")
    print(f"   Learning Rate: {config['training']['lr']}")
    print(f"   Weight Decay: {config['training']['weight_decay']}")
    print(f"   Alpha (seg loss weight): {config['training']['alpha']}")
    
    best_val_acc = 0.0
    best_epoch = 0
    
    for epoch in range(config['training']['epochs']):
        print(f"\n--- Epoch {epoch + 1}/{config['training']['epochs']} ---")
        
        # è®­ç»ƒ
        train_loss, train_acc = train(model, train_loader, optimizer, criterion_label, criterion_seg, config['training']['alpha'])
        
        # éªŒè¯
        val_loss, val_acc, val_preds, val_labels = validate(model, val_loader, criterion_label)

        print(f"[Train] Loss: {train_loss:.4f}, Acc: {train_acc:.4f}")
        print(f"[Val]   Loss: {val_loss:.4f}, Acc: {val_acc:.4f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_epoch = epoch + 1
            torch.save(model.state_dict(), os.path.join(config['log']['save_dir'], "best_model.pth"))
            print(f"ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")

        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % config['log']['save_interval'] == 0:
            torch.save(model.state_dict(), os.path.join(config['log']['save_dir'], f"model_epoch{epoch + 1}.pth"))
            print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: epoch {epoch + 1}")
    
    print(f"\n=== è®­ç»ƒå®Œæˆ ===")
    print(f"ğŸ† æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f} (Epoch {best_epoch})")
    print(f"ğŸ“ æ¨¡å‹ä¿å­˜è·¯å¾„: {config['log']['save_dir']}")
    
    # æœ€ç»ˆè¯„ä¼°
    print(f"\n=== æœ€ç»ˆè¯„ä¼° ===")
    model.load_state_dict(torch.load(os.path.join(config['log']['save_dir'], "best_model.pth")))
    final_loss, final_acc, final_preds, final_labels = validate(model, val_loader, criterion_label)
    print(f"æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {final_acc:.4f}")
    
    # æ‰“å°æ··æ·†çŸ©é˜µï¼ˆå¦‚æœç±»åˆ«ä¸å¤ªå¤šï¼‰
    if len(unique_labels) <= 10:
        try:
            from sklearn.metrics import classification_report
            report = classification_report(final_labels.numpy(), final_preds.numpy(), 
                                         target_names=[f"Class_{i}" for i in unique_labels])
            print("åˆ†ç±»æŠ¥å‘Š:")
            print(report)
        except ImportError:
            print("å®‰è£…sklearnä»¥æŸ¥çœ‹è¯¦ç»†åˆ†ç±»æŠ¥å‘Š: pip install scikit-learn")

if __name__ == "__main__":
    main()