#!/usr/bin/env python3
"""
è¡€ç®¡å›¾æ•°æ®åŠ è½½å™¨ - é€‚é…VesselPreprocessorç”Ÿæˆçš„npzæ ¼å¼æ•°æ®
"""

import os
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from typing import Dict, List, Tuple, Optional
import pickle
from sklearn.model_selection import train_test_split

class VesselGraphDataset(Dataset):
    """è¡€ç®¡å›¾æ•°æ®é›† - é€‚é…é¢„å¤„ç†åçš„npzæ ¼å¼"""
    
    def __init__(self, data_list: List[Dict], transform=None):
        """
        Args:
            data_list: åŒ…å«é¢„å¤„ç†æ•°æ®çš„å­—å…¸åˆ—è¡¨
            transform: æ•°æ®å˜æ¢ï¼ˆå¯é€‰ï¼‰
        """
        self.data_list = data_list
        self.transform = transform
        
    def __len__(self):
        return len(self.data_list)
    
    def __getitem__(self, idx):
        data = self.data_list[idx]
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        sample = {
            'case_id': data['case_id'],
            'node_features': torch.FloatTensor(data['node_features']),  # [N, 54]
            'node_positions': torch.FloatTensor(data['node_positions']),  # [N, 3]
            'edge_index': torch.LongTensor(data['edge_index']),  # [2, E]
            'image_cubes': torch.FloatTensor(data['image_cubes']).unsqueeze(1),  # [N, 1, 32, 32, 32]
            'node_classes': torch.LongTensor(data['node_classes']),  # [N]
            'vessel_node_ranges': data['vessel_node_ranges'],
            'node_to_vessel': data['node_to_vessel']
        }
        
        if self.transform:
            sample = self.transform(sample)
            
        return sample

def collate_fn(batch):
    """è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•° - å¤„ç†ä¸åŒå¤§å°çš„å›¾"""
    # å› ä¸ºæ¯ä¸ªå›¾çš„èŠ‚ç‚¹æ•°ä¸åŒï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
    if len(batch) == 1:
        return batch[0]
    
    # å¯¹äºæ‰¹å¤„ç†ï¼Œæˆ‘ä»¬ç®€å•è¿”å›ç¬¬ä¸€ä¸ªæ ·æœ¬
    # åœ¨å®é™…è®­ç»ƒä¸­ï¼ŒCPR-TaG-Neté€šå¸¸ä½¿ç”¨batch_size=1
    return batch[0]

def load_processed_data(processed_data_dir: str) -> List[Dict]:
    """åŠ è½½æ‰€æœ‰é¢„å¤„ç†åçš„æ•°æ®"""
    data_list = []
    
    if not os.path.exists(processed_data_dir):
        print(f"âŒ Directory {processed_data_dir} does not exist!")
        return data_list
    
    for filename in os.listdir(processed_data_dir):
        if filename.endswith('_processed.npz'):
            filepath = os.path.join(processed_data_dir, filename)
            
            try:
                # åŠ è½½npzæ–‡ä»¶
                loaded_data = np.load(filepath, allow_pickle=True)
                
                # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                data_dict = {
                    'case_id': str(loaded_data['case_id']),
                    'node_features': loaded_data['node_features'],
                    'node_positions': loaded_data['node_positions'], 
                    'edge_index': loaded_data['edge_index'],
                    'image_cubes': loaded_data['image_cubes'],
                    'node_classes': loaded_data['node_classes'],
                    'vessel_node_ranges': loaded_data['vessel_node_ranges'].item(),
                    'node_to_vessel': loaded_data['node_to_vessel']
                }
                
                # éªŒè¯æ•°æ®å®Œæ•´æ€§
                if len(data_dict['node_features']) > 0:
                    data_list.append(data_dict)
                    print(f"âœ… Loaded {filename}: {len(data_dict['node_features'])} nodes, {len(data_dict['vessel_node_ranges'])} vessels")
                    
            except Exception as e:
                print(f"âŒ Error loading {filename}: {e}")
                continue
    
    print(f"\nğŸ“Š Successfully loaded {len(data_list)} cases")
    return data_list

def create_data_splits(data_list: List[Dict], 
                      train_ratio: float = 0.7,
                      val_ratio: float = 0.15,
                      test_ratio: float = 0.15,
                      random_state: int = 42) -> Tuple[List[Dict], List[Dict], List[Dict]]:
    """åˆ›å»ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•æ•°æ®åˆ†å‰²"""
    
    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, "æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ä¸º1"
    
    if len(data_list) < 3:
        print("âš ï¸  æ•°æ®é‡å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œä¸‰åˆ†å‰²ï¼Œå°†ä½¿ç”¨ç®€å•åˆ†å‰²")
        if len(data_list) == 1:
            return data_list, [], []
        elif len(data_list) == 2:
            return [data_list[0]], [data_list[1]], []
    
    # é¦–å…ˆåˆ†å‡ºè®­ç»ƒé›†å’Œä¸´æ—¶é›†
    train_data, temp_data = train_test_split(
        data_list, 
        test_size=(1 - train_ratio),
        random_state=random_state,
        shuffle=True
    )
    
    # ä»ä¸´æ—¶é›†ä¸­åˆ†å‡ºéªŒè¯é›†å’Œæµ‹è¯•é›†
    if len(temp_data) >= 2:
        val_size = val_ratio / (val_ratio + test_ratio)
        val_data, test_data = train_test_split(
            temp_data,
            test_size=(1 - val_size),
            random_state=random_state,
            shuffle=True
        )
    else:
        val_data = temp_data
        test_data = []
    
    print(f"ğŸ“Š Data splits:")
    print(f"  Train: {len(train_data)} cases")
    print(f"  Val: {len(val_data)} cases") 
    print(f"  Test: {len(test_data)} cases")
    
    return train_data, val_data, test_data

def create_dataloaders(train_data: List[Dict], 
                      val_data: List[Dict], 
                      test_data: List[Dict],
                      batch_size: int = 1,
                      num_workers: int = 2) -> Tuple[DataLoader, DataLoader, DataLoader]:
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    
    train_dataset = VesselGraphDataset(train_data)
    val_dataset = VesselGraphDataset(val_data) if val_data else None
    test_dataset = VesselGraphDataset(test_data) if test_data else None
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=True if torch.cuda.is_available() else False
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=1,  # éªŒè¯æ—¶ä½¿ç”¨batch_size=1
        shuffle=False,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=True if torch.cuda.is_available() else False
    ) if val_dataset else None
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=1,
        shuffle=False,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=True if torch.cuda.is_available() else False
    ) if test_dataset else None
    
    return train_loader, val_loader, test_loader

def save_data_splits(train_data: List[Dict], val_data: List[Dict], test_data: List[Dict], save_dir: str):
    """ä¿å­˜æ•°æ®åˆ†å‰²ç»“æœ"""
    os.makedirs(save_dir, exist_ok=True)
    
    # ä¿å­˜ä¸ºpickleæ ¼å¼ï¼ˆä¸åŸä»£ç å…¼å®¹ï¼‰
    with open(os.path.join(save_dir, 'train_data.pkl'), 'wb') as f:
        pickle.dump(train_data, f)
    
    with open(os.path.join(save_dir, 'val_data.pkl'), 'wb') as f:
        pickle.dump(val_data, f)
        
    with open(os.path.join(save_dir, 'test_data.pkl'), 'wb') as f:
        pickle.dump(test_data, f)
    
    print(f"ğŸ’¾ Data splits saved to {save_dir}")

def load_data_splits(save_dir: str) -> Tuple[List[Dict], List[Dict], List[Dict]]:
    """åŠ è½½ä¿å­˜çš„æ•°æ®åˆ†å‰²"""
    with open(os.path.join(save_dir, 'train_data.pkl'), 'rb') as f:
        train_data = pickle.load(f)
    
    with open(os.path.join(save_dir, 'val_data.pkl'), 'rb') as f:
        val_data = pickle.load(f)
        
    with open(os.path.join(save_dir, 'test_data.pkl'), 'rb') as f:
        test_data = pickle.load(f)
        
    return train_data, val_data, test_data

def main():
    """æµ‹è¯•æ•°æ®åŠ è½½å™¨"""
    print("ğŸ”„ Loading processed data...")
    data_list = load_processed_data('/home/lihe/classify/lungmap/data/processed')
    
    if len(data_list) == 0:
        print("âŒ No data found! Please run vessel_preprocessing.py first.")
        return
    
    print("\nğŸ“Š Creating data splits...")
    train_data, val_data, test_data = create_data_splits(data_list)
    
    print("\nğŸ’¾ Saving data splits...")
    save_data_splits(train_data, val_data, test_data, '/home/lihe/classify/lungmap/data/splits')
    
    print("\nğŸ”§ Creating data loaders...")
    train_loader, val_loader, test_loader = create_dataloaders(train_data, val_data, test_data)
    
    print("\nâœ… Testing data loader...")
    for i, batch in enumerate(train_loader):
        print(f"Batch {i+1}:")
        print(f"  Case ID: {batch['case_id']}")
        print(f"  Node features: {batch['node_features'].shape}")
        print(f"  Node positions: {batch['node_positions'].shape}")
        print(f"  Edge index: {batch['edge_index'].shape}")
        print(f"  Image cubes: {batch['image_cubes'].shape}")
        print(f"  Node classes: {batch['node_classes'].shape}")
        print(f"  Unique classes: {torch.unique(batch['node_classes']).tolist()}")
        print()
        
        if i >= 2:  # åªæµ‹è¯•å‰3ä¸ªæ‰¹æ¬¡
            break
    
    print("ğŸ‰ Data loader setup completed!")

if __name__ == "__main__":
    main()
