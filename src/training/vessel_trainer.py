#!/usr/bin/env python3
"""
CPR-TaG-Net è¡€ç®¡åˆ†æè®­ç»ƒç®¡é“
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
import numpy as np
from tqdm import tqdm
import json
from datetime import datetime
import argparse
import sys

# å¯¼å…¥æ•°æ®åŠ è½½å™¨
from vessel_data_loader import load_processed_data, create_data_splits, create_dataloaders, save_data_splits, load_data_splits

# å¯¼å…¥CPR-TaG-Netæ¨¡å‹
sys.path.append('.')
try:
    from data_loader import VesselGraphDataset as OriginalDataset
    print("âœ… Found original data_loader.py")
except ImportError:
    print("âš ï¸  Original data_loader.py not found, using vessel_data_loader")

class VesselTrainer:
    """è¡€ç®¡å›¾åƒåˆ†æè®­ç»ƒå™¨"""
    
    def __init__(self, config: dict):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆ›å»ºå®éªŒç›®å½•
        self.exp_dir = os.path.join('experiments', config['exp_name'])
        os.makedirs(self.exp_dir, exist_ok=True)
        os.makedirs(os.path.join(self.exp_dir, 'checkpoints'), exist_ok=True)
        os.makedirs(os.path.join(self.exp_dir, 'logs'), exist_ok=True)
        
        # åˆå§‹åŒ–tensorboard
        self.writer = SummaryWriter(os.path.join(self.exp_dir, 'logs'))
        
        # ä¿å­˜é…ç½®
        with open(os.path.join(self.exp_dir, 'config.json'), 'w') as f:
            json.dump(config, f, indent=2)
        
        print(f"ğŸ”§ Experiment directory: {self.exp_dir}")
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹"""
        # ç®€åŒ–çš„æ¨¡å‹å®šä¹‰ï¼Œå¦‚æœæ‰¾ä¸åˆ°åŸå§‹æ¨¡å‹
        try:
            # å°è¯•å¯¼å…¥åŸå§‹CPR-TaG-Net
            from train_eval import build_model
            self.model = build_model().to(self.device)
            print("âœ… Using original CPR-TaG-Net model")
        except ImportError:
            # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªç®€åŒ–çš„æ›¿ä»£æ¨¡å‹
            print("âš ï¸  Original model not found, creating simplified model")
            self.model = self._create_simplified_model().to(self.device)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        print(f"ğŸ”§ Model Setup:")
        print(f"  Total parameters: {total_params:,}")
        print(f"  Trainable parameters: {trainable_params:,}")
        print(f"  Device: {self.device}")
    
    def _create_simplified_model(self):
        """åˆ›å»ºç®€åŒ–çš„æ¨¡å‹ç”¨äºæµ‹è¯•"""
        class SimplifiedVesselNet(nn.Module):
            def __init__(self, num_classes=18, node_feature_dim=54):
                super().__init__()
                
                # å›¾åƒç¼–ç å™¨ (ç®€åŒ–ç‰ˆ3D CNN)
                self.image_encoder = nn.Sequential(
                    nn.Conv3d(1, 32, kernel_size=3, padding=1),
                    nn.ReLU(),
                    nn.AdaptiveAvgPool3d((4, 4, 4)),
                    nn.Conv3d(32, 64, kernel_size=3, padding=1),
                    nn.ReLU(),
                    nn.AdaptiveAvgPool3d((1, 1, 1)),
                    nn.Flatten()
                )
                
                # èŠ‚ç‚¹ç‰¹å¾ç¼–ç å™¨
                self.node_encoder = nn.Sequential(
                    nn.Linear(node_feature_dim, 128),
                    nn.ReLU(),
                    nn.Linear(128, 256)
                )
                
                # åˆ†ç±»å™¨
                self.classifier = nn.Sequential(
                    nn.Linear(64 + 256, 512),
                    nn.ReLU(),
                    nn.Dropout(0.5),
                    nn.Linear(512, 256),
                    nn.ReLU(),
                    nn.Dropout(0.5),
                    nn.Linear(256, num_classes)
                )
            
            def forward(self, node_features, node_positions, edge_index, image_cubes):
                # å›¾åƒç‰¹å¾æå–
                batch_size = image_cubes.size(0)
                image_feat = self.image_encoder(image_cubes)  # [N, 64]
                
                # èŠ‚ç‚¹ç‰¹å¾ç¼–ç 
                node_feat = self.node_encoder(node_features)  # [N, 256]
                
                # ç‰¹å¾èåˆ
                combined_feat = torch.cat([image_feat, node_feat], dim=1)  # [N, 320]
                
                # åˆ†ç±»
                logits = self.classifier(combined_feat)  # [N, num_classes]
                
                return logits
        
        return SimplifiedVesselNet()
    
    def setup_optimizer_scheduler(self):
        """è®¾ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨"""
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=self.config['weight_decay']
        )
        
        self.scheduler = optim.lr_scheduler.StepLR(
            self.optimizer,
            step_size=self.config['lr_step_size'],
            gamma=self.config['lr_gamma']
        )
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss(ignore_index=-1)
        
        print(f"ğŸ”§ Optimizer: Adam (lr={self.config['learning_rate']})")
    
    def setup_data(self):
        """è®¾ç½®æ•°æ®"""
        print("ğŸ”„ Loading data...")
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ•°æ®åˆ†å‰²
        if os.path.exists('data_splits/train_data.pkl'):
            print("Found existing data splits, loading...")
            train_data, val_data, test_data = load_data_splits('data_splits')
        else:
            print("Creating new data splits...")
            data_list = load_processed_data('processed_data')
            if len(data_list) == 0:
                raise ValueError("No processed data found! Please run vessel_preprocessing.py first.")
            
            train_data, val_data, test_data = create_data_splits(data_list)
            save_data_splits(train_data, val_data, test_data, 'data_splits')
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.train_loader, self.val_loader, self.test_loader = create_dataloaders(
            train_data, val_data, test_data,
            batch_size=self.config['batch_size'],
            num_workers=self.config['num_workers']
        )
        
        print(f"ğŸ“Š Data loaded:")
        print(f"  Train batches: {len(self.train_loader)}")
        print(f"  Val batches: {len(self.val_loader) if self.val_loader else 0}")
        print(f"  Test batches: {len(self.test_loader) if self.test_loader else 0}")
    
    def train_epoch(self, epoch: int):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        total_loss = 0.0
        correct_predictions = 0
        total_predictions = 0
        
        pbar = tqdm(self.train_loader, desc=f'Train Epoch {epoch}')
        
        for batch_idx, batch in enumerate(pbar):
            try:
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                node_features = batch['node_features'].to(self.device)
                node_positions = batch['node_positions'].to(self.device)
                edge_index = batch['edge_index'].to(self.device)
                image_cubes = batch['image_cubes'].to(self.device)
                node_classes = batch['node_classes'].to(self.device)
                
                # å‰å‘ä¼ æ’­
                self.optimizer.zero_grad()
                
                logits = self.model(node_features, node_positions, edge_index, image_cubes)
                loss = self.criterion(logits, node_classes)
                
                # åå‘ä¼ æ’­
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                self.optimizer.step()
                
                # ç»Ÿè®¡
                total_loss += loss.item()
                predictions = torch.argmax(logits, dim=1)
                correct_predictions += (predictions == node_classes).sum().item()
                total_predictions += node_classes.size(0)
                
                # æ›´æ–°è¿›åº¦æ¡
                current_acc = correct_predictions / total_predictions
                pbar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Acc': f'{current_acc:.4f}'
                })
                
                # è®°å½•åˆ°tensorboard
                global_step = epoch * len(self.train_loader) + batch_idx
                self.writer.add_scalar('Train/BatchLoss', loss.item(), global_step)
                self.writer.add_scalar('Train/BatchAcc', current_acc, global_step)
                
            except Exception as e:
                print(f"âŒ Error in batch {batch_idx}: {e}")
                continue
        
        avg_loss = total_loss / len(self.train_loader) if len(self.train_loader) > 0 else 0
        avg_acc = correct_predictions / total_predictions if total_predictions > 0 else 0
        
        return avg_loss, avg_acc
    
    def validate(self, epoch: int):
        """éªŒè¯"""
        if not self.val_loader:
            return 0.0, 0.0
            
        self.model.eval()
        
        total_loss = 0.0
        correct_predictions = 0
        total_predictions = 0
        
        with torch.no_grad():
            pbar = tqdm(self.val_loader, desc=f'Val Epoch {epoch}')
            
            for batch in pbar:
                try:
                    # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                    node_features = batch['node_features'].to(self.device)
                    node_positions = batch['node_positions'].to(self.device)
                    edge_index = batch['edge_index'].to(self.device)
                    image_cubes = batch['image_cubes'].to(self.device)
                    node_classes = batch['node_classes'].to(self.device)
                    
                    logits = self.model(node_features, node_positions, edge_index, image_cubes)
                    loss = self.criterion(logits, node_classes)
                    
                    total_loss += loss.item()
                    predictions = torch.argmax(logits, dim=1)
                    correct_predictions += (predictions == node_classes).sum().item()
                    total_predictions += node_classes.size(0)
                    
                    pbar.set_postfix({
                        'Loss': f'{loss.item():.4f}',
                        'Acc': f'{correct_predictions/total_predictions:.4f}'
                    })
                    
                except Exception as e:
                    print(f"âŒ Error in validation: {e}")
                    continue
        
        avg_loss = total_loss / len(self.val_loader) if len(self.val_loader) > 0 else 0
        avg_acc = correct_predictions / total_predictions if total_predictions > 0 else 0
        
        return avg_loss, avg_acc
    
    def save_checkpoint(self, epoch: int, train_loss: float, val_loss: float, val_acc: float, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'train_loss': train_loss,
            'val_loss': val_loss,
            'val_acc': val_acc,
            'config': self.config
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        torch.save(checkpoint, os.path.join(self.exp_dir, 'checkpoints', 'latest.pth'))
        
        # ä¿å­˜æœ€ä½³æ£€æŸ¥ç‚¹
        if is_best:
            torch.save(checkpoint, os.path.join(self.exp_dir, 'checkpoints', 'best.pth'))
        
        # å®šæœŸä¿å­˜
        if epoch % self.config['save_interval'] == 0:
            torch.save(checkpoint, os.path.join(self.exp_dir, 'checkpoints', f'epoch_{epoch}.pth'))
    
    def load_checkpoint(self, checkpoint_path: str):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        return checkpoint['epoch'], checkpoint.get('val_acc', 0.0)
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print("ğŸš€ Starting training...")
        
        best_val_acc = 0.0
        start_epoch = 1
        
        # å¦‚æœæœ‰æ£€æŸ¥ç‚¹ï¼ŒåŠ è½½å®ƒ
        latest_checkpoint = os.path.join(self.exp_dir, 'checkpoints', 'latest.pth')
        if os.path.exists(latest_checkpoint) and self.config['resume']:
            print("Resuming from checkpoint...")
            start_epoch, best_val_acc = self.load_checkpoint(latest_checkpoint)
            start_epoch += 1
        
        for epoch in range(start_epoch, self.config['epochs'] + 1):
            print(f"\n{'='*50}")
            print(f"Epoch {epoch}/{self.config['epochs']}")
            print(f"{'='*50}")
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(epoch)
            
            # éªŒè¯
            val_loss, val_acc = self.validate(epoch)
            
            # è°ƒåº¦å™¨æ­¥è¿›
            self.scheduler.step()
            
            # è®°å½•åˆ°tensorboard
            self.writer.add_scalar('Epoch/TrainLoss', train_loss, epoch)
            self.writer.add_scalar('Epoch/TrainAcc', train_acc, epoch)
            self.writer.add_scalar('Epoch/ValLoss', val_loss, epoch)
            self.writer.add_scalar('Epoch/ValAcc', val_acc, epoch)
            self.writer.add_scalar('Epoch/LR', self.scheduler.get_last_lr()[0], epoch)
            
            # æ‰“å°ç»“æœ
            print(f"\nEpoch {epoch} Results:")
            print(f"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
            print(f"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
            print(f"  Learning Rate: {self.scheduler.get_last_lr()[0]:.6f}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            is_best = val_acc > best_val_acc
            if is_best:
                best_val_acc = val_acc
                print(f"  ğŸ‰ New best validation accuracy: {best_val_acc:.4f}")
            
            self.save_checkpoint(epoch, train_loss, val_loss, val_acc, is_best)
        
        print(f"\nğŸ‰ Training completed!")
        print(f"Best validation accuracy: {best_val_acc:.4f}")
        
        self.writer.close()

def get_default_config():
    """è·å–é»˜è®¤é…ç½®"""
    return {
        'exp_name': f'vessel_training_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
        'epochs': 50,
        'batch_size': 1,
        'learning_rate': 0.001,
        'weight_decay': 1e-4,
        'lr_step_size': 20,
        'lr_gamma': 0.5,
        'num_workers': 2,
        'save_interval': 10,
        'resume': False,
    }

def main():
    parser = argparse.ArgumentParser(description='Train CPR-TaG-Net for vessel analysis')
    parser.add_argument('--config', type=str, help='Path to config file')
    parser.add_argument('--exp_name', type=str, help='Experiment name')
    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs')
    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')
    parser.add_argument('--resume', action='store_true', help='Resume from checkpoint')
    
    args = parser.parse_args()
    
    # è·å–é…ç½®
    config = get_default_config()
    
    if args.config and os.path.exists(args.config):
        with open(args.config, 'r') as f:
            config.update(json.load(f))
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    if args.exp_name:
        config['exp_name'] = args.exp_name
    if args.epochs:
        config['epochs'] = args.epochs
    if args.lr:
        config['learning_rate'] = args.lr
    if args.resume:
        config['resume'] = True
    
    print("ğŸ”§ Configuration:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = VesselTrainer(config)
    trainer.setup_model()
    trainer.setup_optimizer_scheduler()
    trainer.setup_data()
    trainer.train()

if __name__ == "__main__":
    main()
