#!/usr/bin/env python3
"""
è¡€ç®¡åˆ†ç±»æ¨¡å‹è®­ç»ƒå¯åŠ¨è„šæœ¬ - æ•´åˆæ•°æ®åŠ è½½å’ŒCPR-TaG-Netè®­ç»ƒ
"""

import os
import sys
import argparse
import time
from tqdm import tqdm
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from txt_logger import create_txt_logger  # æ›¿æ¢TensorBoard

# æ·»åŠ æ¨¡å—è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src', 'training'))
sys.path.append(os.path.join(os.path.dirname(__file__), 'src', 'models', 'CPR_TaG_Net'))

from vessel_data_loader import load_processed_data, create_data_splits, create_dataloaders
from models.cpr_tagnet import CPRTaGNet

class VesselTrainer:
    """è¡€ç®¡åˆ†ç±»è®­ç»ƒå™¨"""
    
    def __init__(self, args):
        self.args = args
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ Using device: {self.device}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(args.checkpoint_dir, exist_ok=True)
        os.makedirs(args.log_dir, exist_ok=True)
        
        # åˆ›å»ºå¯è§†åŒ–ç›®å½•ï¼ˆå¦‚æœå¯ç”¨å¢å¼ºåŠŸèƒ½ï¼‰
        if args.enable_visualization or args.save_confusion_matrix or args.save_training_curves:
            os.makedirs(args.visualization_dir, exist_ok=True)
            print(f"ğŸ“Š å¯è§†åŒ–ç›®å½•: {args.visualization_dir}")
        
        # è®¾ç½®TXTæ—¥å¿—è®°å½•å™¨
        self.logger = create_txt_logger(args.log_dir, "cpr_tagnet_training")
        
        # è®°å½•é…ç½®ä¿¡æ¯
        config_dict = {
            "model": "CPR-TaG-Net",
            "epochs": args.epochs,
            "learning_rate": args.learning_rate,
            "node_batch_size": args.node_batch_size,
            "weight_decay": args.weight_decay,
            "max_nodes_per_case": args.max_nodes_per_case,
            "enable_large_cases": args.enable_large_cases,
            "device": str(self.device),
            "enhanced_features": {
                "graph_completion": getattr(args, 'enable_graph_completion', False),
                "visualization": getattr(args, 'enable_visualization', False),
                "confusion_matrix": getattr(args, 'save_confusion_matrix', False),
                "training_curves": getattr(args, 'save_training_curves', False)
            }
        }
        self.logger.log_config(config_dict)
        
        # ğŸ”§ è¡€ç®¡æ„ŸçŸ¥è®­ç»ƒæ”¹è¿›ï¼šæ­£ç¡®çš„è¡€ç®¡å±‚æ¬¡ä¿¡æ¯ï¼ˆ15ç±»3çº§ç»“æ„ï¼‰
        self.vessel_hierarchy = {
            # ä¸€çº§ï¼šä¸»è‚ºåŠ¨è„‰
            'MPA': {'level': 0, 'parent': None, 'expected_class_range': [0]},
            
            # äºŒçº§ï¼šå·¦å³è‚ºåŠ¨è„‰
            'LPA': {'level': 1, 'parent': 'MPA', 'expected_class_range': [1]},
            'RPA': {'level': 1, 'parent': 'MPA', 'expected_class_range': [2]},
            
            # ä¸‰çº§ï¼šå·¦ä¾§åˆ†æ”¯
            'Lupper': {'level': 2, 'parent': 'LPA', 'expected_class_range': [3]},
            'L1+2': {'level': 2, 'parent': 'LPA', 'expected_class_range': [5]},        # å·¦ä¸Šå¶å˜å¼‚
            'L1+3': {'level': 2, 'parent': 'LPA', 'expected_class_range': [7]},        # å·¦ä¸Šå¶å˜å¼‚  
            'Linternal': {'level': 2, 'parent': 'LPA', 'expected_class_range': [9]},
            'Lmedium': {'level': 2, 'parent': 'LPA', 'expected_class_range': [11]},    # å·¦ä¸­å¶ï¼ˆå˜å¼‚ï¼‰
            'Ldown': {'level': 2, 'parent': 'LPA', 'expected_class_range': [13]},
            
            # ä¸‰çº§ï¼šå³ä¾§åˆ†æ”¯
            'Rupper': {'level': 2, 'parent': 'RPA', 'expected_class_range': [4]},
            'R1+2': {'level': 2, 'parent': 'RPA', 'expected_class_range': [6]},        # å³ä¸Šå¶å˜å¼‚
            'R1+3': {'level': 2, 'parent': 'RPA', 'expected_class_range': [8]},        # å³ä¸Šå¶å˜å¼‚
            'Rinternal': {'level': 2, 'parent': 'RPA', 'expected_class_range': [10]},
            'Rmedium': {'level': 2, 'parent': 'RPA', 'expected_class_range': [12]},    # å³ä¸­å¶
            'RDown': {'level': 2, 'parent': 'RPA', 'expected_class_range': [14]}
        }
        
        # è¡€ç®¡ç±»å‹åµŒå…¥ï¼ˆæ›´æ–°ç»´åº¦ä»¥é€‚åº”æ›´å¤šè¡€ç®¡ç±»å‹ï¼‰
        self.vessel_type_embedding = nn.Embedding(len(self.vessel_hierarchy) + 1, 32).to(self.device)  # +1 for unknown vessels
        
        # ğŸ”§ è¡€ç®¡æ„ŸçŸ¥è®­ç»ƒæƒé‡é…ç½®
        self.vessel_consistency_weight = getattr(args, 'vessel_consistency_weight', 0.1)
        self.spatial_consistency_weight = getattr(args, 'spatial_consistency_weight', 0.05)
        self.enable_vessel_aware = getattr(args, 'enable_vessel_aware', True)
        
        # åˆå§‹åŒ–å¢å¼ºè®­ç»ƒå·¥å…·
        self.enhanced_trainer = None
        if any([args.enable_graph_completion, args.enable_visualization, 
                args.save_confusion_matrix, args.save_training_curves]):
            try:
                from enhanced_training_utils import create_enhanced_trainer
                self.enhanced_trainer = create_enhanced_trainer(args.visualization_dir)
                print("ğŸ§  å¢å¼ºè®­ç»ƒåŠŸèƒ½å·²å¯ç”¨")
            except ImportError as e:
                print(f"âš ï¸ æ— æ³•å¯¼å…¥å¢å¼ºè®­ç»ƒå·¥å…·: {e}")
                print("   éƒ¨åˆ†å¯è§†åŒ–åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")
        
        # ğŸ”§ åŠ¨æ€éªŒè¯é›†é…ç½®
        self.dynamic_split_interval = getattr(args, 'dynamic_split_interval', 10)  # æ¯10ä¸ªepoché‡æ–°åˆ†å‰²
        self.current_split_epoch = 0
        self.original_data_list = None  # ä¿å­˜åŸå§‹æ•°æ®ç”¨äºé‡æ–°åˆ†å‰²
        
        # ğŸ”§ å¤šé‡éªŒè¯é…ç½®
        self.enable_cross_validation = getattr(args, 'enable_cross_validation', False)
        self.cv_folds = getattr(args, 'cv_folds', 5)
        self.enable_leave_one_out = getattr(args, 'enable_leave_one_out', False)
        self.cv_results = []  # å­˜å‚¨äº¤å‰éªŒè¯ç»“æœ
        
        # è®­ç»ƒå†å²è®°å½•ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        self.train_losses = []
        self.train_accs = []
        self.val_losses = []
        self.val_accs = []
        
        # è®­ç»ƒå¼€å§‹æ—¶é—´
        self.start_time = time.time()
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._setup_model()
        
        # åŠ è½½æ•°æ®
        self._setup_data()
        
        # è®¾ç½®è®­ç»ƒå‚æ•°
        self._setup_training()
    
    def _setup_model(self):
        """åˆå§‹åŒ–CPR-TaG-Netæ¨¡å‹"""
        print("ğŸ”§ Setting up CPR-TaG-Net model...")
        
        # ğŸ”§ è®¡ç®—å¢å¼ºåçš„ç‰¹å¾ç»´åº¦
        # åŸå§‹ç‰¹å¾(54) + è¡€ç®¡ç±»å‹åµŒå…¥(32) + å±‚æ¬¡ç¼–ç (3) + è¡€ç®¡å†…ä½ç½®(1) = 90
        enhanced_feature_dim = 54 + 32 + 3 + 1
        
        # CPR-TaG-Net é…ç½®å‚æ•°
        model_config = {
            'num_classes': 15,  # å®é™…æ•°æ®ä¸­æœ‰0-14å…±15ä¸ªç±»åˆ«ï¼ˆåŒ…æ‹¬èƒŒæ™¯ç±»0ï¼‰
            'node_feature_dim': enhanced_feature_dim,  # å¢å¼ºåçš„èŠ‚ç‚¹ç‰¹å¾ç»´åº¦
            'image_channels': 1,  # å›¾åƒé€šé“æ•°
        }
        
        self.model = CPRTaGNet(**model_config).to(self.device)
        print(f"âœ… CPR-TaG-Net initialized with enhanced features ({enhanced_feature_dim}D)")
        print(f"âœ… Model has {sum(p.numel() for p in self.model.parameters()):,} parameters")
        print(f"ğŸ©¸ è¡€ç®¡å±‚æ¬¡ç»“æ„: {len(self.vessel_hierarchy)} ç§è¡€ç®¡ç±»å‹")
        
    def _setup_data(self):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨ - 24GBæ˜¾å­˜ä¼˜åŒ–ç‰ˆæœ¬"""
        print("ğŸ“Š Loading and preparing data...")
        
        # åŠ è½½é¢„å¤„ç†æ•°æ®
        data_list = load_processed_data(self.args.data_dir)
        print(f"âœ… Loaded {len(data_list)} cases")
        
        # æ ¹æ®æ˜¾å­˜æƒ…å†µç­›é€‰æ•°æ®
        if self.args.enable_large_cases:
            # 24GBæ˜¾å­˜ï¼šä½¿ç”¨å¤§éƒ¨åˆ†æ•°æ®
            filtered_data = []
            for data in data_list:
                nodes = len(data['node_features'])
                if nodes < self.args.max_nodes_per_case:
                    filtered_data.append(data)
                    print(f"  âœ… {data['case_id']}: {nodes} nodes")
                else:
                    print(f"  âš ï¸  {data['case_id']}: {nodes} nodes (è·³è¿‡ï¼Œè¶…è¿‡{self.args.max_nodes_per_case})")
            
            print(f"ğŸ“Š Using {len(filtered_data)} cases (24GBæ˜¾å­˜ä¼˜åŒ–)")
        else:
            # ä¿å®ˆæ¨¡å¼ï¼šåªä½¿ç”¨å°æ•°æ®é›†
            filtered_data = []
            for data in data_list:
                if len(data['node_features']) < self.args.max_nodes:
                    filtered_data.append(data)
                    print(f"  âœ… {data['case_id']}: {len(data['node_features'])} nodes")
            
            print(f"ğŸ“Š Selected {len(filtered_data)} small cases for training")
        
        if len(filtered_data) == 0:
            print("âŒ No suitable cases found, using first 3 cases")
            filtered_data = data_list[:3]
        
        # ğŸ”§ ä¿å­˜åŸå§‹æ•°æ®ç”¨äºåŠ¨æ€é‡æ–°åˆ†å‰²
        self.original_data_list = filtered_data.copy()
        
        # åˆ›å»ºåˆå§‹æ•°æ®åˆ†å‰²
        self._create_dynamic_data_splits(random_seed=42)
        
        print(f"âœ… Data splits - Train: {len(self.train_data)}, Val: {len(self.val_data)}, Test: {len(self.test_data)}")
        
        # æ˜¾å­˜ä½¿ç”¨é¢„ä¼°
        total_train_nodes = sum(len(d['node_features']) for d in self.train_data)
        estimated_memory_gb = total_train_nodes * 0.25 / 1024  # æ¯èŠ‚ç‚¹çº¦0.25MB
        print(f"ğŸ’¾ Estimated training memory: {estimated_memory_gb:.1f}GB")
        
    def _setup_training(self):
        """è®¾ç½®è®­ç»ƒå‚æ•°"""
        print("ğŸ”§ Setting up training parameters...")
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss()
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=self.args.learning_rate,
            weight_decay=self.args.weight_decay
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.StepLR(
            self.optimizer,
            step_size=self.args.step_size,
            gamma=self.args.gamma
        )
        
        print("âœ… Training setup completed")
    
    def _create_dynamic_data_splits(self, random_seed=None):
        """ğŸ”§ åŠ¨æ€åˆ›å»ºæ•°æ®åˆ†å‰² - å®šæœŸé‡æ–°åˆ†å‰²é˜²æ­¢éªŒè¯é›†è¿‡æ‹Ÿåˆ"""
        if random_seed is None:
            import time
            random_seed = int(time.time()) % 10000  # ä½¿ç”¨æ—¶é—´æˆ³ç”Ÿæˆéšæœºç§å­
        
        print(f"ğŸ”„ Creating dynamic data splits (seed: {random_seed})...")
        
        # åˆ›å»ºæ•°æ®åˆ†å‰²
        train_data, val_data, test_data = create_data_splits(
            self.original_data_list, 
            train_ratio=0.7, 
            val_ratio=0.15, 
            test_ratio=0.15,
            random_state=random_seed
        )
        
        # ä¿å­˜ä¸ºç±»å±æ€§
        self.train_data = train_data
        self.val_data = val_data
        self.test_data = test_data
        
        # è®°å½•åˆ†å‰²ä¿¡æ¯
        train_case_ids = [d['case_id'] for d in train_data]
        val_case_ids = [d['case_id'] for d in val_data]
        test_case_ids = [d['case_id'] for d in test_data]
        
        self.logger.log_message(f"åŠ¨æ€æ•°æ®é‡æ–°åˆ†å‰² (seed: {random_seed})")
        self.logger.log_message(f"è®­ç»ƒé›†: {train_case_ids}")
        self.logger.log_message(f"éªŒè¯é›†: {val_case_ids}")
        self.logger.log_message(f"æµ‹è¯•é›†: {test_case_ids}")
        
        print(f"ğŸ”„ Dynamic split completed - Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}")
    
    def _should_resplit_data(self, epoch):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åˆ†å‰²æ•°æ®"""
        if self.dynamic_split_interval <= 0:
            return False
            
        return (epoch + 1) % self.dynamic_split_interval == 0 and epoch > 0
    
    def _perform_cross_validation(self, epoch):
        """ğŸ”§ æ‰§è¡ŒK-foldäº¤å‰éªŒè¯"""
        if not self.enable_cross_validation or not self.original_data_list:
            return
        
        print(f"\nğŸ”¬ æ‰§è¡Œ {self.cv_folds}-fold äº¤å‰éªŒè¯ (Epoch {epoch + 1})...")
        
        from sklearn.model_selection import KFold
        import numpy as np
        
        kf = KFold(n_splits=self.cv_folds, shuffle=True, random_state=42 + epoch)
        fold_results = []
        
        for fold, (train_idx, val_idx) in enumerate(kf.split(self.original_data_list)):
            # ç®€åŒ–è¾“å‡ºï¼šä¸å†ä¸ºæ¯ä¸ªfoldæ‰“å°è¯¦ç»†ä¿¡æ¯
            if fold == 0:  # åªåœ¨ç¬¬ä¸€ä¸ªfoldæ—¶è¾“å‡ºæç¤º
                print(f"  ğŸ“ æ‰§è¡Œ {self.cv_folds} ä¸ª folds...")
            
            # åˆ›å»ºfoldæ•°æ®é›†
            fold_train_data = [self.original_data_list[i] for i in train_idx]
            fold_val_data = [self.original_data_list[i] for i in val_idx]
            
            # ä¸´æ—¶ä¿å­˜å½“å‰æ•°æ®é›†
            original_train = self.train_data
            original_val = self.val_data
            
            # è®¾ç½®foldæ•°æ®é›†
            self.train_data = fold_train_data
            self.val_data = fold_val_data
            
            try:
                # åœ¨å½“å‰foldä¸ŠéªŒè¯
                val_loss, val_acc = self.validate(epoch)
                fold_results.append({
                    'fold': fold + 1,
                    'val_loss': val_loss,
                    'val_acc': val_acc,
                    'train_cases': len(fold_train_data),
                    'val_cases': len(fold_val_data)
                })
                
                # ç®€åŒ–è¾“å‡ºï¼šåªæ˜¾ç¤ºå…³é”®ä¿¡æ¯ï¼Œä¸é€ä¸ªfoldæ‰“å°
                
            except Exception as e:
                # ç®€åŒ–é”™è¯¯è¾“å‡º
                fold_results.append({
                    'fold': fold + 1,
                    'val_loss': float('inf'),
                    'val_acc': 0.0,
                    'error': str(e)
                })
            
            # æ¢å¤åŸå§‹æ•°æ®é›†
            self.train_data = original_train
            self.val_data = original_val
        
        # è®¡ç®—äº¤å‰éªŒè¯ç»Ÿè®¡
        valid_results = [r for r in fold_results if 'error' not in r]
        if valid_results:
            avg_loss = np.mean([r['val_loss'] for r in valid_results])
            avg_acc = np.mean([r['val_acc'] for r in valid_results])
            std_loss = np.std([r['val_loss'] for r in valid_results])
            std_acc = np.std([r['val_acc'] for r in valid_results])
            
            cv_result = {
                'epoch': epoch + 1,
                'folds': self.cv_folds,
                'avg_loss': avg_loss,
                'avg_acc': avg_acc,
                'std_loss': std_loss,
                'std_acc': std_acc,
                'fold_results': fold_results
            }
            
            self.cv_results.append(cv_result)
            
            print(f"  ğŸ“Š äº¤å‰éªŒè¯ç»“æœ:")
            print(f"    å¹³å‡éªŒè¯æŸå¤±: {avg_loss:.4f} Â± {std_loss:.4f}")
            print(f"    å¹³å‡éªŒè¯å‡†ç¡®ç‡: {avg_acc:.2f}% Â± {std_acc:.2f}%")
            
            # è®°å½•åˆ°æ—¥å¿—
            self.logger.add_scalar('CrossVal/AvgLoss', avg_loss, epoch)
            self.logger.add_scalar('CrossVal/AvgAccuracy', avg_acc, epoch)
            self.logger.add_scalar('CrossVal/StdLoss', std_loss, epoch)
            self.logger.add_scalar('CrossVal/StdAccuracy', std_acc, epoch)
            
            self.logger.log_message(f"K-foldäº¤å‰éªŒè¯ (Epoch {epoch + 1}): å¹³å‡å‡†ç¡®ç‡ {avg_acc:.2f}% Â± {std_acc:.2f}%")
        else:
            print(f"  âŒ æ‰€æœ‰foldéªŒè¯éƒ½å¤±è´¥äº†")
    
    def _perform_leave_one_out_validation(self, epoch):
        """ğŸ”§ æ‰§è¡Œç•™ä¸€æ³•éªŒè¯"""
        if not self.enable_leave_one_out or not self.original_data_list:
            return
        
        if len(self.original_data_list) > 10:
            print(f"  âš ï¸ æ•°æ®é›†è¿‡å¤§({len(self.original_data_list)}ä¸ªæ¡ˆä¾‹)ï¼Œè·³è¿‡ç•™ä¸€æ³•éªŒè¯")
            return
        
        print(f"\nğŸ”¬ æ‰§è¡Œç•™ä¸€æ³•éªŒè¯ (Epoch {epoch + 1})...")
        
        loo_results = []
        print(f"  ğŸ“ æ‰§è¡Œç•™ä¸€æ³•éªŒè¯ ({len(self.original_data_list)} ä¸ªæ¡ˆä¾‹)...")
        
        for i, test_case in enumerate(self.original_data_list):
            # ç®€åŒ–è¾“å‡ºï¼šä¸ä¸ºæ¯ä¸ªæ¡ˆä¾‹å•ç‹¬æ‰“å°
            
            # åˆ›å»ºç•™ä¸€æ³•æ•°æ®é›†
            loo_train_data = [self.original_data_list[j] for j in range(len(self.original_data_list)) if j != i]
            loo_test_data = [test_case]
            
            # ä¸´æ—¶ä¿å­˜å½“å‰æ•°æ®é›†
            original_train = self.train_data
            original_val = self.val_data
            
            # è®¾ç½®ç•™ä¸€æ³•æ•°æ®é›†
            self.train_data = loo_train_data
            self.val_data = loo_test_data
            
            try:
                # åœ¨å½“å‰ç•™ä¸€æ³•è®¾ç½®ä¸ŠéªŒè¯
                val_loss, val_acc = self.validate(epoch)
                loo_results.append({
                    'test_case': test_case['case_id'],
                    'val_loss': val_loss,
                    'val_acc': val_acc
                })
                
                # ç®€åŒ–è¾“å‡ºï¼šä¸ä¸ºæ¯ä¸ªæ¡ˆä¾‹å•ç‹¬æ‰“å°ç»“æœ
                
            except Exception as e:
                # ç®€åŒ–é”™è¯¯è¾“å‡ºï¼šåªè®°å½•åˆ°ç»“æœä¸­
                loo_results.append({
                    'test_case': test_case['case_id'],
                    'val_loss': float('inf'),
                    'val_acc': 0.0,
                    'error': str(e)
                })
            
            # æ¢å¤åŸå§‹æ•°æ®é›†
            self.train_data = original_train
            self.val_data = original_val
        
        # è®¡ç®—ç•™ä¸€æ³•ç»Ÿè®¡
        valid_results = [r for r in loo_results if 'error' not in r]
        if valid_results:
            import numpy as np
            avg_loss = np.mean([r['val_loss'] for r in valid_results])
            avg_acc = np.mean([r['val_acc'] for r in valid_results])
            std_loss = np.std([r['val_loss'] for r in valid_results])
            std_acc = np.std([r['val_acc'] for r in valid_results])
            
            print(f"  ğŸ“Š ç•™ä¸€æ³•éªŒè¯ç»“æœ:")
            print(f"    å¹³å‡éªŒè¯æŸå¤±: {avg_loss:.4f} Â± {std_loss:.4f}")
            print(f"    å¹³å‡éªŒè¯å‡†ç¡®ç‡: {avg_acc:.2f}% Â± {std_acc:.2f}%")
            
            # è®°å½•åˆ°æ—¥å¿—
            self.logger.add_scalar('LeaveOneOut/AvgLoss', avg_loss, epoch)
            self.logger.add_scalar('LeaveOneOut/AvgAccuracy', avg_acc, epoch)
            
            self.logger.log_message(f"ç•™ä¸€æ³•éªŒè¯ (Epoch {epoch + 1}): å¹³å‡å‡†ç¡®ç‡ {avg_acc:.2f}% Â± {std_acc:.2f}%")
        else:
            print(f"  âŒ æ‰€æœ‰ç•™ä¸€æ³•éªŒè¯éƒ½å¤±è´¥äº†")
    
    def train_on_case(self, case_data, epoch, case_idx):
        """æ”¹è¿›çš„è¡€ç®¡æ„ŸçŸ¥è®­ç»ƒæ–¹æ³• - å……åˆ†åˆ©ç”¨è¡€ç®¡è¿æ¥å‰ç½®ä¿¡æ¯"""
        self.model.train()
        
        case_id = case_data['case_id']
        vessel_ranges = case_data['vessel_node_ranges']
        
        # å‡†å¤‡æ•°æ®
        node_features = torch.FloatTensor(case_data['node_features']).to(self.device)
        node_positions = torch.FloatTensor(case_data['node_positions']).to(self.device)
        edge_index = torch.LongTensor(case_data['edge_index']).to(self.device)
        image_cubes = torch.FloatTensor(case_data['image_cubes']).to(self.device)
        node_classes = torch.LongTensor(case_data['node_classes']).to(self.device)
        
        total_loss = 0.0
        total_correct = 0
        total_samples = 0
        
        # ğŸ”§ å…³é”®æ”¹è¿›1: æŒ‰è¡€ç®¡å±‚æ¬¡é¡ºåºè®­ç»ƒ
        vessel_order = self._get_hierarchical_vessel_order(vessel_ranges)
        
        for vessel_name in vessel_order:
            if vessel_name not in vessel_ranges:
                continue
                
            start, end = vessel_ranges[vessel_name]
            vessel_node_indices = torch.arange(start, end + 1, device=self.device)
            
            # ğŸ”§ å…³é”®æ”¹è¿›2: è¡€ç®¡å†…æ‰¹å¤„ç†ï¼Œä¿æŒè¿ç»­æ€§
            vessel_batches = self._create_vessel_batches(vessel_node_indices, max_batch_size=200)
            
            for batch_idx, batch_indices in enumerate(vessel_batches):
                try:
                    # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
                    batch_node_features = node_features[batch_indices]
                    batch_node_positions = node_positions[batch_indices]
                    batch_image_cubes = image_cubes[batch_indices]
                    batch_node_classes = node_classes[batch_indices]
                    
                    # ğŸ”§ å…³é”®æ”¹è¿›3: æ³¨å…¥è¡€ç®¡å…ˆéªŒä¿¡æ¯
                    enhanced_features = self._inject_vessel_context(
                        batch_node_features, vessel_name, batch_indices, vessel_ranges
                    )
                    
                    # ğŸ”§ å…³é”®æ”¹è¿›4: è·å–å®Œæ•´è¾¹è¿æ¥ï¼ˆè¡€ç®¡å†…+è¡€ç®¡é—´ï¼‰
                    batch_edge_index = self._get_complete_vessel_edges(
                        edge_index, batch_indices, vessel_ranges, vessel_name
                    )
                    
                    # å‰å‘ä¼ æ’­
                    self.optimizer.zero_grad()
                    outputs = self.model(
                        enhanced_features,
                        batch_node_positions,
                        batch_edge_index,
                        batch_image_cubes
                    )
                    
                    # ğŸ”§ å…³é”®æ”¹è¿›5: å±‚æ¬¡åŒ–æŸå¤±å‡½æ•°
                    loss = self._compute_hierarchical_loss(
                        outputs, batch_node_classes, vessel_name, batch_indices
                    )
                    
                    # åå‘ä¼ æ’­
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    self.optimizer.step()
                    
                    # ç»Ÿè®¡
                    total_loss += loss.item()
                    _, predicted = outputs.max(1)
                    total_correct += predicted.eq(batch_node_classes).sum().item()
                    total_samples += batch_node_classes.size(0)
                    
                    # è®°å½•åˆ°æ—¥å¿— - å‡å°‘è®°å½•é¢‘ç‡ï¼Œé¿å…è¾“å‡ºè¿‡äºé¢‘ç¹
                    if batch_idx % 50 == 0:  # ä»æ¯10ä¸ªbatchæ”¹ä¸ºæ¯50ä¸ªbatchè®°å½•ä¸€æ¬¡
                        global_step = epoch * 1000 + case_idx * 100 + batch_idx
                        self.logger.add_scalar('Train/BatchLoss', loss.item(), global_step)
                    
                    # å†…å­˜æ¸…ç†
                    del enhanced_features, batch_edge_index, outputs
                    torch.cuda.empty_cache()
                    
                except RuntimeError as e:
                    if "out of memory" in str(e):
                        torch.cuda.empty_cache()
                        print(f"âš ï¸  {case_id} OOM in vessel {vessel_name}, skipping batch {batch_idx}")
                        continue
                    else:
                        print(f"âš ï¸  Error in {case_id}, vessel {vessel_name}: {e}")
                        continue
        
        # æ¸…ç†
        del node_features, node_positions, edge_index, image_cubes, node_classes
        torch.cuda.empty_cache()
        
        avg_loss = total_loss / max(1, len(vessel_order))
        accuracy = 100.0 * total_correct / max(1, total_samples)
        
        return avg_loss, accuracy, total_samples
    
    def _get_hierarchical_vessel_order(self, vessel_ranges):
        """è·å–è¡€ç®¡å±‚æ¬¡è®­ç»ƒé¡ºåº"""
        available_vessels = list(vessel_ranges.keys())
        
        # æŒ‰å±‚æ¬¡æ’åºï¼šä¸»å¹² â†’ åˆ†æ”¯
        ordered_vessels = []
        for level in range(3):  # 0-2çº§ï¼ˆä¿®æ­£ä¸º15ç±»3å±‚ç»“æ„ï¼‰
            level_vessels = [
                vessel for vessel in available_vessels 
                if vessel in self.vessel_hierarchy and 
                self.vessel_hierarchy[vessel]['level'] == level
            ]
            # åŒçº§å†…æŒ‰å­—æ¯æ’åºä¿è¯ä¸€è‡´æ€§
            ordered_vessels.extend(sorted(level_vessels))
        
        # æ·»åŠ æœªåœ¨å±‚æ¬¡ä¸­çš„è¡€ç®¡
        remaining = [v for v in available_vessels if v not in ordered_vessels]
        ordered_vessels.extend(sorted(remaining))
        
        return ordered_vessels
    
    def _create_vessel_batches(self, vessel_indices, max_batch_size=200):
        """åˆ›å»ºè¡€ç®¡å†…æ‰¹æ¬¡ï¼Œä¿æŒç©ºé—´è¿ç»­æ€§"""
        if len(vessel_indices) <= max_batch_size:
            return [vessel_indices]
        
        # æŒ‰ç©ºé—´ä½ç½®æ’åºï¼Œä¿æŒè¿ç»­æ€§
        batches = []
        for i in range(0, len(vessel_indices), max_batch_size):
            end_idx = min(i + max_batch_size, len(vessel_indices))
            batches.append(vessel_indices[i:end_idx])
        
        return batches
    
    def _inject_vessel_context(self, node_features, vessel_name, batch_indices, vessel_ranges):
        """æ³¨å…¥è¡€ç®¡ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        batch_size = node_features.shape[0]
        
        # 1. è¡€ç®¡ç±»å‹åµŒå…¥
        vessel_id = self._get_vessel_type_id(vessel_name)
        vessel_embedding = self.vessel_type_embedding(torch.tensor(vessel_id, device=self.device))
        vessel_emb_expanded = vessel_embedding.unsqueeze(0).expand(batch_size, -1)
        
        # 2. å±‚æ¬¡ä½ç½®ç¼–ç 
        hierarchy_encoding = self._compute_hierarchy_encoding(vessel_name, batch_size)
        
        # 3. è¡€ç®¡å†…ä½ç½®ç¼–ç 
        intra_vessel_encoding = self._compute_intra_vessel_position(
            batch_indices, vessel_ranges[vessel_name], batch_size
        )
        
        # 4. ç‰¹å¾èåˆ
        enhanced_features = torch.cat([
            node_features,
            vessel_emb_expanded,
            hierarchy_encoding,
            intra_vessel_encoding
        ], dim=1)
        
        return enhanced_features
    
    def _get_complete_vessel_edges(self, edge_index, batch_indices, vessel_ranges, current_vessel):
        """è·å–å®Œæ•´çš„è¡€ç®¡è¾¹è¿æ¥ä¿¡æ¯"""
        device = edge_index.device
        
        if edge_index.shape[1] == 0:
            return torch.zeros((2, 0), dtype=torch.long, device=device)
        
        # 1. æ‰¹å†…è¾¹ï¼ˆè¡€ç®¡å†…è¿æ¥ï¼‰
        src_in_batch = torch.isin(edge_index[0], batch_indices)
        dst_in_batch = torch.isin(edge_index[1], batch_indices)
        intra_batch_mask = src_in_batch & dst_in_batch
        intra_edges = edge_index[:, intra_batch_mask]
        
        # 2. è¡€ç®¡é—´è¿æ¥ï¼ˆå…³é”®æ”¹è¿›ï¼ï¼‰
        inter_vessel_edges = self._get_inter_vessel_connections(
            edge_index, batch_indices, vessel_ranges, current_vessel
        )
        
        # 3. åˆå¹¶è¾¹ä¿¡æ¯
        if inter_vessel_edges.shape[1] > 0:
            all_edges = torch.cat([intra_edges, inter_vessel_edges], dim=1)
        else:
            all_edges = intra_edges
        
        # 4. é‡æ–°ç´¢å¼•
        if all_edges.shape[1] > 0:
            return self._reindex_edges_for_batch(all_edges, batch_indices)
        else:
            return torch.zeros((2, 0), dtype=torch.long, device=device)
    
    def _get_inter_vessel_connections(self, edge_index, batch_indices, vessel_ranges, current_vessel):
        """è·å–è¡€ç®¡é—´çš„å…³é”®è¿æ¥"""
        device = edge_index.device
        
        # è·å–å½“å‰è¡€ç®¡çš„çˆ¶è¡€ç®¡å’Œå­è¡€ç®¡
        parent_vessel = self.vessel_hierarchy.get(current_vessel, {}).get('parent')
        child_vessels = [
            v for v, info in self.vessel_hierarchy.items() 
            if info.get('parent') == current_vessel
        ]
        
        relevant_vessels = []
        if parent_vessel and parent_vessel in vessel_ranges:
            relevant_vessels.append(parent_vessel)
        for child in child_vessels:
            if child in vessel_ranges:
                relevant_vessels.append(child)
        
        if not relevant_vessels:
            return torch.zeros((2, 0), dtype=torch.long, device=device)
        
        # æ”¶é›†ç›¸å…³è¡€ç®¡çš„èŠ‚ç‚¹
        relevant_nodes = []
        for vessel in relevant_vessels:
            start, end = vessel_ranges[vessel]
            relevant_nodes.extend(range(start, end + 1))
        
        if not relevant_nodes:
            return torch.zeros((2, 0), dtype=torch.long, device=device)
        
        relevant_nodes_tensor = torch.tensor(relevant_nodes, device=device)
        
        # æ‰¾åˆ°è·¨è¡€ç®¡çš„è¾¹è¿æ¥
        src_in_batch = torch.isin(edge_index[0], batch_indices)
        dst_in_relevant = torch.isin(edge_index[1], relevant_nodes_tensor)
        src_in_relevant = torch.isin(edge_index[0], relevant_nodes_tensor)
        dst_in_batch = torch.isin(edge_index[1], batch_indices)
        
        # åŒå‘è¿æ¥ï¼šbatch->relevant æˆ– relevant->batch
        inter_mask = (src_in_batch & dst_in_relevant) | (src_in_relevant & dst_in_batch)
        
        return edge_index[:, inter_mask]
    
    def _compute_hierarchical_loss(self, outputs, targets, vessel_name, batch_indices):
        """è®¡ç®—å±‚æ¬¡åŒ–æŸå¤±å‡½æ•°"""
        # 1. åŸºç¡€äº¤å‰ç†µæŸå¤±
        ce_loss = F.cross_entropy(outputs, targets)
        
        # 2. è¡€ç®¡ç±»å‹ä¸€è‡´æ€§æŸå¤±
        vessel_consistency_loss = self._compute_vessel_consistency_loss(
            outputs, targets, vessel_name
        )
        
        # 3. ç©ºé—´è¿ç»­æ€§æŸå¤±
        spatial_consistency_loss = self._compute_spatial_consistency_loss(
            outputs, batch_indices
        )
        
        # æƒé‡ç»„åˆ
        total_loss = (ce_loss + 
                     self.vessel_consistency_weight * vessel_consistency_loss + 
                     self.spatial_consistency_weight * spatial_consistency_loss)
        
        return total_loss
    
    def _get_vessel_type_id(self, vessel_name):
        """è·å–è¡€ç®¡ç±»å‹ID"""
        vessel_names = list(self.vessel_hierarchy.keys())
        if vessel_name in vessel_names:
            return vessel_names.index(vessel_name)
        else:
            return len(vessel_names)  # æœªçŸ¥è¡€ç®¡ç±»å‹
    
    def _compute_hierarchy_encoding(self, vessel_name, batch_size):
        """è®¡ç®—å±‚æ¬¡ä½ç½®ç¼–ç """
        if vessel_name in self.vessel_hierarchy:
            level = self.vessel_hierarchy[vessel_name]['level']
        else:
            level = -1  # æœªçŸ¥å±‚æ¬¡
        
        # ç®€å•çš„ä½ç½®ç¼–ç ï¼ˆ3çº§ï¼š0-ä¸»å¹²ï¼Œ1-å·¦å³åˆ†æ”¯ï¼Œ2-æœ«ç«¯åˆ†æ”¯ï¼‰
        encoding = torch.zeros(batch_size, 3, device=self.device)  # 3å±‚
        if 0 <= level < 3:
            encoding[:, level] = 1.0
        
        return encoding
    
    def _compute_intra_vessel_position(self, batch_indices, vessel_range, batch_size):
        """è®¡ç®—è¡€ç®¡å†…ä½ç½®ç¼–ç """
        start, end = vessel_range
        vessel_length = end - start + 1
        
        # å½’ä¸€åŒ–ä½ç½®
        positions = (batch_indices - start).float() / max(1, vessel_length - 1)
        position_encoding = positions.unsqueeze(1)  # [batch_size, 1]
        
        return position_encoding
    
    def _compute_vessel_consistency_loss(self, outputs, targets, vessel_name):
        """è¡€ç®¡ç±»å‹ä¸€è‡´æ€§æŸå¤±"""
        if vessel_name not in self.vessel_hierarchy:
            return torch.tensor(0.0, device=outputs.device)
        
        expected_classes = self.vessel_hierarchy[vessel_name]['expected_class_range']
        
        # é¢„æµ‹æ¦‚ç‡
        probs = F.softmax(outputs, dim=1)
        
        # æœŸæœ›ç±»åˆ«çš„æ¦‚ç‡å’Œ
        expected_prob = probs[:, expected_classes].sum(dim=1)
        
        # ä¸€è‡´æ€§æŸå¤±ï¼šé¼“åŠ±é¢„æµ‹åœ¨æœŸæœ›èŒƒå›´å†…
        consistency_loss = -torch.log(expected_prob + 1e-8).mean()
        
        return consistency_loss
    
    def _compute_spatial_consistency_loss(self, outputs, batch_indices):
        """ç©ºé—´è¿ç»­æ€§æŸå¤±ï¼ˆç›¸é‚»èŠ‚ç‚¹é¢„æµ‹åº”è¯¥ç›¸ä¼¼ï¼‰"""
        if len(batch_indices) < 2:
            return torch.tensor(0.0, device=outputs.device)
        
        # ç›¸é‚»èŠ‚ç‚¹çš„é¢„æµ‹åº”è¯¥ç›¸ä¼¼
        pred_probs = F.softmax(outputs, dim=1)
        
        # è®¡ç®—ç›¸é‚»é¢„æµ‹çš„å·®å¼‚
        neighbor_diff = torch.abs(pred_probs[1:] - pred_probs[:-1]).sum(dim=1)
        
        # è¿ç»­æ€§æŸå¤±
        continuity_loss = neighbor_diff.mean()
        
        return continuity_loss
    
    def _reindex_edges_for_batch(self, edges, batch_indices):
        """ä¸ºæ‰¹æ¬¡é‡æ–°ç´¢å¼•è¾¹"""
        device = edges.device
        
        # åˆ›å»ºç´¢å¼•æ˜ å°„
        max_idx = max(edges.max().item(), batch_indices.max().item())
        old_to_new = torch.full((max_idx + 1,), -1, device=device, dtype=torch.long)
        old_to_new[batch_indices] = torch.arange(len(batch_indices), device=device)
        
        # é‡æ–°ç´¢å¼•
        new_edges = edges.clone()
        new_edges[0] = old_to_new[edges[0]]
        new_edges[1] = old_to_new[edges[1]]
        
        # è¿‡æ»¤æ— æ•ˆè¾¹
        valid_mask = (new_edges[0] >= 0) & (new_edges[1] >= 0)
        valid_edges = new_edges[:, valid_mask]
        
        return valid_edges
    
    def _prepare_batch_edges(self, edge_index, batch_indices, batch_size):
        """ä¸ºæ‰¹å¤„ç†å‡†å¤‡è¾¹è¿æ¥ - ä¿ç•™åŸæ–¹æ³•ä½œä¸ºå¤‡ç”¨"""
        device = edge_index.device
        
        # å¦‚æœæ²¡æœ‰è¾¹ï¼Œè¿”å›ç©ºè¾¹ç´¢å¼•
        if edge_index.shape[1] == 0:
            return torch.zeros((2, 0), dtype=torch.long, device=device)
        
        # åˆ›å»ºç´¢å¼•æ˜ å°„
        max_idx = max(edge_index.max().item(), batch_indices.max().item())
        old_to_new = torch.full((max_idx + 1,), -1, device=device)
        old_to_new[batch_indices] = torch.arange(len(batch_indices), device=device)
        
        # æ‰¾åˆ°æ‰¹å†…çš„è¾¹
        src_in_batch = torch.isin(edge_index[0], batch_indices)
        dst_in_batch = torch.isin(edge_index[1], batch_indices)
        edge_mask = src_in_batch & dst_in_batch
        
        if edge_mask.sum() == 0:
            # æ²¡æœ‰æ‰¹å†…è¾¹ï¼Œè¿”å›ç©ºè¾¹ç´¢å¼•
            return torch.zeros((2, 0), dtype=torch.long, device=device)
        
        # è·å–æ‰¹å†…è¾¹å¹¶é‡æ–°ç´¢å¼•
        batch_edges = edge_index[:, edge_mask]
        
        # å®‰å…¨åœ°é‡æ–°ç´¢å¼•
        try:
            batch_edges[0] = old_to_new[batch_edges[0]]
            batch_edges[1] = old_to_new[batch_edges[1]]
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ— æ•ˆçš„ç´¢å¼•
            if (batch_edges < 0).any() or (batch_edges >= len(batch_indices)).any():
                print(f"âš ï¸  Invalid edge indices detected, returning empty edge index")
                return torch.zeros((2, 0), dtype=torch.long, device=device)
            
            return batch_edges
            
        except Exception as e:
            print(f"âš ï¸  Error in edge preparation: {e}, returning empty edge index")
            return torch.zeros((2, 0), dtype=torch.long, device=device)

    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch - åŸºäºcaseçš„æ‰¹å¤„ç†"""
        print(f"\n{'='*50}")
        print(f"Epoch {epoch+1}/{self.args.epochs}")
        print(f"{'='*50}")
        
        epoch_loss = 0.0
        epoch_correct = 0
        epoch_samples = 0
        
        # éšæœºæ‰“ä¹±è®­ç»ƒæ•°æ®
        import random
        train_indices = list(range(len(self.train_data)))
        random.shuffle(train_indices)
        
        pbar = tqdm(train_indices, desc='Training Cases')
        for i, case_idx in enumerate(pbar):
            case_data = self.train_data[case_idx]
            case_id = case_data['case_id']
            num_nodes = len(case_data['node_features'])
            
            try:
                loss, acc, samples = self.train_on_case(case_data, epoch, i)
                
                epoch_loss += loss * samples
                epoch_correct += acc * samples / 100.0
                epoch_samples += samples
                
                current_acc = 100.0 * epoch_correct / epoch_samples if epoch_samples > 0 else 0.0
                
                # ç®€åŒ–è¿›åº¦æ¡æ›´æ–°ï¼šåªåœ¨é‡è¦èŠ‚ç‚¹æ›´æ–°ï¼Œå‡å°‘è¾“å‡ºé¢‘ç‡
                if i % 3 == 0 or i == len(train_indices) - 1:  # æ¯3ä¸ªæ¡ˆä¾‹æ›´æ–°ä¸€æ¬¡ï¼Œæˆ–æœ€åä¸€ä¸ªæ¡ˆä¾‹
                    pbar.set_postfix({
                        'Case': case_id,
                        'Nodes': num_nodes,
                        'Loss': f'{loss:.3f}',
                        'Acc': f'{current_acc:.1f}%',
                        'GPU': f'{torch.cuda.memory_allocated()/1024**3:.1f}GB'
                    })
                
            except Exception as e:
                print(f"âŒ Error training on {case_id}: {e}")
                continue
        
        avg_loss = epoch_loss / epoch_samples if epoch_samples > 0 else 0.0
        avg_acc = 100.0 * epoch_correct / epoch_samples if epoch_samples > 0 else 0.0
        
        # è®°å½•åˆ°æ—¥å¿—
        self.logger.add_scalar('Train/EpochLoss', avg_loss, epoch)
        self.logger.add_scalar('Train/EpochAccuracy', avg_acc, epoch)
        
        return avg_loss, avg_acc
    
    def validate_on_case(self, case_data):
        """æ”¹è¿›çš„è¡€ç®¡æ„ŸçŸ¥éªŒè¯æ–¹æ³•"""
        self.model.eval()
        
        case_id = case_data['case_id']
        vessel_ranges = case_data['vessel_node_ranges']
        
        with torch.no_grad():
            # å‡†å¤‡æ•°æ®
            node_features = torch.FloatTensor(case_data['node_features']).to(self.device)
            node_positions = torch.FloatTensor(case_data['node_positions']).to(self.device)
            edge_index = torch.LongTensor(case_data['edge_index']).to(self.device)
            image_cubes = torch.FloatTensor(case_data['image_cubes']).to(self.device)
            node_classes = torch.LongTensor(case_data['node_classes']).to(self.device)
            
            total_loss = 0.0
            total_correct = 0
            total_samples = 0
            
            # ğŸ”§ éªŒè¯æ—¶ä¹Ÿä½¿ç”¨è¡€ç®¡æ„ŸçŸ¥çš„æ–¹æ³•
            vessel_order = self._get_hierarchical_vessel_order(vessel_ranges)
            
            for vessel_name in vessel_order:
                if vessel_name not in vessel_ranges:
                    continue
                    
                start, end = vessel_ranges[vessel_name]
                vessel_node_indices = torch.arange(start, end + 1, device=self.device)
                
                vessel_batches = self._create_vessel_batches(vessel_node_indices, max_batch_size=300)
                
                for batch_idx, batch_indices in enumerate(vessel_batches):
                    try:
                        # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
                        batch_node_features = node_features[batch_indices]
                        batch_node_positions = node_positions[batch_indices]
                        batch_image_cubes = image_cubes[batch_indices]
                        batch_node_classes = node_classes[batch_indices]
                        
                        # æ³¨å…¥è¡€ç®¡ä¸Šä¸‹æ–‡ä¿¡æ¯
                        enhanced_features = self._inject_vessel_context(
                            batch_node_features, vessel_name, batch_indices, vessel_ranges
                        )
                        
                        # è·å–å®Œæ•´è¾¹è¿æ¥
                        batch_edge_index = self._get_complete_vessel_edges(
                            edge_index, batch_indices, vessel_ranges, vessel_name
                        )
                        
                        # å‰å‘ä¼ æ’­
                        outputs = self.model(
                            enhanced_features,
                            batch_node_positions,
                            batch_edge_index,
                            batch_image_cubes
                        )
                        
                        # ğŸ”§ ç»Ÿä¸€æŸå¤±å‡½æ•°ï¼šéªŒè¯æ—¶ä¹Ÿä½¿ç”¨å±‚çº§æŸå¤±
                        loss = self._compute_hierarchical_loss(
                            outputs, batch_node_classes, vessel_name, batch_indices
                        )
                        
                        # ç»Ÿè®¡
                        total_loss += loss.item()
                        _, predicted = outputs.max(1)
                        total_correct += predicted.eq(batch_node_classes).sum().item()
                        total_samples += batch_node_classes.size(0)
                        
                        # æ¸…ç†å†…å­˜
                        del enhanced_features, batch_edge_index, outputs
                        torch.cuda.empty_cache()
                        
                    except RuntimeError as e:
                        if "out of memory" in str(e):
                            torch.cuda.empty_cache()
                            print(f"âš ï¸  Validation {case_id} OOM in vessel {vessel_name}, skipping batch {batch_idx}")
                            continue
                        else:
                            print(f"âš ï¸  Validation error in {case_id}, vessel {vessel_name}: {e}")
                            continue
            
            # æ¸…ç†
            del node_features, node_positions, edge_index, image_cubes, node_classes
            torch.cuda.empty_cache()
            
            avg_loss = total_loss / max(1, len(vessel_order))
            accuracy = 100.0 * total_correct / max(1, total_samples)
            
            return avg_loss, accuracy, total_samples

    def validate(self, epoch):
        """éªŒè¯ä¸€ä¸ªepoch"""
        if not self.val_data:
            return 0.0, 0.0
        
        print("\nğŸ” Validating...")
        
        epoch_loss = 0.0
        epoch_correct = 0
        epoch_samples = 0
        
        pbar = tqdm(self.val_data, desc='Validation Cases')
        for i, case_data in enumerate(pbar):
            case_id = case_data['case_id']
            num_nodes = len(case_data['node_features'])
            
            try:
                loss, acc, samples = self.validate_on_case(case_data)
                
                epoch_loss += loss * samples
                epoch_correct += acc * samples / 100.0
                epoch_samples += samples
                
                current_acc = 100.0 * epoch_correct / epoch_samples if epoch_samples > 0 else 0.0
                
                # ç®€åŒ–éªŒè¯è¿›åº¦æ¡æ›´æ–°ï¼šå‡å°‘æ›´æ–°é¢‘ç‡
                if i % 2 == 0 or i == len(self.val_data) - 1:  # æ¯2ä¸ªæ¡ˆä¾‹æ›´æ–°ä¸€æ¬¡ï¼Œæˆ–æœ€åä¸€ä¸ªæ¡ˆä¾‹
                    pbar.set_postfix({
                        'Case': case_id,
                        'Nodes': num_nodes,
                        'Loss': f'{loss:.3f}',
                        'Acc': f'{current_acc:.1f}%'
                    })
                
            except Exception as e:
                print(f"âŒ Error validating on {case_id}: {e}")
                continue
        
        avg_loss = epoch_loss / epoch_samples if epoch_samples > 0 else 0.0
        avg_acc = 100.0 * epoch_correct / epoch_samples if epoch_samples > 0 else 0.0
        
        # è®°å½•åˆ°æ—¥å¿—
        self.logger.add_scalar('Val/EpochLoss', avg_loss, epoch)
        self.logger.add_scalar('Val/EpochAccuracy', avg_acc, epoch)
        
        return avg_loss, avg_acc
    
    def _generate_confusion_matrix(self, epoch):
        """ç”ŸæˆéªŒè¯é›†çš„æ··æ·†çŸ©é˜µ"""
        if not self.val_data or not self.enhanced_trainer:
            return
        
        print("ğŸ“Š æ”¶é›†éªŒè¯æ•°æ®ç”¨äºæ··æ·†çŸ©é˜µ...")
        all_preds = []
        all_labels = []
        
        self.model.eval()
        with torch.no_grad():
            for case_data in self.val_data:
                try:
                    # è·å–æ¡ˆä¾‹æ•°æ®
                    node_features = torch.tensor(case_data['node_features'], dtype=torch.float32).to(self.device)
                    node_positions = torch.tensor(case_data['node_positions'], dtype=torch.float32).to(self.device)
                    edge_index = torch.tensor(case_data['edge_index'], dtype=torch.long).to(self.device)
                    image_cubes = torch.tensor(case_data['image_cubes'], dtype=torch.float32).to(self.device)  # [N, 32, 32, 32]
                    labels = torch.tensor(case_data['node_classes'], dtype=torch.long).to(self.device)
                    
                    # æ¨¡å‹é¢„æµ‹ - ä½¿ç”¨CPR-TaG-Netçš„å®Œæ•´å‚æ•°
                    logits = self.model(node_features, node_positions, edge_index, image_cubes)
                    
                    preds = torch.argmax(logits, dim=1)
                    
                    # å›¾å½¢è¡¥å…¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if self.args.enable_graph_completion and node_positions is not None:
                        try:
                            refined_preds, _ = self.enhanced_trainer.complete_graph(
                                node_positions, preds, distance_threshold=5.0
                            )
                            preds = refined_preds
                        except Exception as e:
                            print(f"   âš ï¸ å›¾å½¢è¡¥å…¨å¤±è´¥: {e}")
                    
                    all_preds.append(preds.cpu())
                    all_labels.append(labels.cpu())
                    
                except Exception as e:
                    print(f"   âš ï¸ å¤„ç†æ¡ˆä¾‹ {case_data.get('case_id', 'unknown')} å¤±è´¥: {e}")
                    continue
        
        if all_preds:
            # åˆå¹¶æ‰€æœ‰é¢„æµ‹å’Œæ ‡ç­¾
            all_preds = torch.cat(all_preds)
            all_labels = torch.cat(all_labels)
            
            # ç”Ÿæˆæ··æ·†çŸ©é˜µ
            self.enhanced_trainer.plot_confusion_matrix(all_labels, all_preds, epoch)
            
            # åˆ†æé¢„æµ‹è´¨é‡
            analysis = self.enhanced_trainer.analyze_prediction_quality(all_labels, all_preds)
            self.enhanced_trainer.save_analysis_report(analysis, epoch)
        else:
            print("   âš ï¸ æ— æ³•æ”¶é›†åˆ°éªŒè¯æ•°æ®")
    
    def save_checkpoint(self, epoch, train_loss, val_loss, val_acc, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'train_loss': train_loss,
            'val_loss': val_loss,
            'val_accuracy': val_acc,
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        checkpoint_path = os.path.join(self.args.checkpoint_dir, 'latest.pth')
        torch.save(checkpoint, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = os.path.join(self.args.checkpoint_dir, 'best.pth')
            torch.save(checkpoint, best_path)
            print(f"ğŸ’¾ Best model saved at epoch {epoch+1}")
    
    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        print("ğŸš€ Starting CPR-TaG-Net training...")
        
        # è®°å½•è®­ç»ƒå¼€å§‹ä¿¡æ¯
        model_info = f"CPR-TaG-Net ({sum(p.numel() for p in self.model.parameters()):,} å‚æ•°)"
        data_info = f"è®­ç»ƒé›†: {len(self.train_data)} æ¡ˆä¾‹, éªŒè¯é›†: {len(self.val_data) if self.val_data else 0} æ¡ˆä¾‹"
        config_info = f"Epochs: {self.args.epochs}, Learning Rate: {self.args.learning_rate}, Batch Size: {self.args.node_batch_size}"
        
        self.logger.log_training_start(model_info, data_info, config_info)
        
        # æ˜¾ç¤ºå¢å¼ºåŠŸèƒ½çŠ¶æ€
        if self.enhanced_trainer:
            print("ğŸ§  å¢å¼ºåŠŸèƒ½å·²å¯ç”¨:")
            enhanced_features = []
            if self.args.enable_graph_completion:
                enhanced_features.append("å›¾å½¢è¡¥å…¨")
                print("  âœ… å›¾å½¢è¡¥å…¨")
            if self.args.enable_visualization:
                enhanced_features.append("è®­ç»ƒå¯è§†åŒ–")
                print("  âœ… è®­ç»ƒå¯è§†åŒ–")
            if self.args.save_confusion_matrix:
                enhanced_features.append("æ··æ·†çŸ©é˜µ")
                print("  âœ… æ··æ·†çŸ©é˜µ")
            if self.args.save_training_curves:
                enhanced_features.append("è®­ç»ƒæ›²çº¿")
                print("  âœ… è®­ç»ƒæ›²çº¿")
            
            self.logger.log_message(f"å¢å¼ºåŠŸèƒ½å·²å¯ç”¨: {', '.join(enhanced_features)}")
        
        best_val_acc = 0.0
        best_epoch = 0
        
        for epoch in range(self.args.epochs):
            epoch_start_time = time.time()
            
            # ğŸ”§ åŠ¨æ€æ•°æ®é‡æ–°åˆ†å‰²
            if self._should_resplit_data(epoch):
                print(f"\nğŸ”„ Epoch {epoch + 1}: æ‰§è¡ŒåŠ¨æ€æ•°æ®é‡æ–°åˆ†å‰²...")
                self._create_dynamic_data_splits()
                self.logger.log_message(f"Epoch {epoch + 1}: æ‰§è¡ŒåŠ¨æ€æ•°æ®é‡æ–°åˆ†å‰²")
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(epoch)
            
            # éªŒè¯
            val_loss, val_acc = self.validate(epoch)
            
            # ğŸ”§ å¤šé‡éªŒè¯ï¼šäº¤å‰éªŒè¯ï¼ˆæ¯5ä¸ªepochæ‰§è¡Œä¸€æ¬¡ï¼‰
            if (epoch + 1) % 5 == 0:
                if self.enable_cross_validation:
                    self._perform_cross_validation(epoch)
                
                if self.enable_leave_one_out:
                    self._perform_leave_one_out_validation(epoch)
            
            # è®°å½•è®­ç»ƒå†å²ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
            self.train_losses.append(train_loss)
            self.train_accs.append(train_acc)
            if val_loss is not None:
                self.val_losses.append(val_loss)
                self.val_accs.append(val_acc)
            
            # æ›´æ–°å­¦ä¹ ç‡
            current_lr = self.optimizer.param_groups[0]['lr']
            self.scheduler.step()
            new_lr = self.optimizer.param_groups[0]['lr']
            
            # è®°å½•å­¦ä¹ ç‡å˜åŒ–
            self.logger.add_scalar('Train/LearningRate', new_lr, epoch)
            
            # è®¡ç®—epochç”¨æ—¶
            epoch_time = time.time() - epoch_start_time
            
            # æ‰“å°ç»“æœ
            print(f"\nğŸ“Š Epoch {epoch+1} Results:")
            print(f"  Training   - Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%")
            if self.val_data:
                print(f"  Validation - Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%")
            print(f"  Learning Rate: {new_lr:.6f}")
            print(f"  Epoch Time: {epoch_time:.1f}s")
            
            # è®°å½•è¯¦ç»†çš„epochä¿¡æ¯åˆ°æ—¥å¿—
            extra_info = f"Epochç”¨æ—¶: {epoch_time:.1f}s"
            if current_lr != new_lr:
                extra_info += f", å­¦ä¹ ç‡å˜åŒ–: {current_lr:.6f} -> {new_lr:.6f}"
            
            self.logger.log_epoch_summary(
                epoch + 1, train_loss, train_acc, 
                val_loss if val_loss is not None else 0.0, 
                val_acc if val_acc is not None else 0.0, 
                new_lr, extra_info
            )
            
            # å¢å¼ºåŠŸèƒ½ï¼šç”Ÿæˆè®­ç»ƒè¿›åº¦å¯è§†åŒ–
            if self.enhanced_trainer and self.args.save_training_curves and (epoch + 1) % 5 == 0:
                try:
                    self.enhanced_trainer.visualize_training_progress(
                        self.train_losses, self.train_accs, 
                        self.val_losses if self.val_losses else None,
                        self.val_accs if self.val_accs else None,
                        epoch + 1
                    )
                    self.logger.log_message(f"ç”Ÿæˆè®­ç»ƒè¿›åº¦å¯è§†åŒ– (Epoch {epoch + 1})")
                except Exception as e:
                    print(f"   âš ï¸ è®­ç»ƒè¿›åº¦å¯è§†åŒ–å¤±è´¥: {e}")
                    self.logger.log_message(f"è®­ç»ƒè¿›åº¦å¯è§†åŒ–å¤±è´¥: {e}", "WARNING")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            is_best = val_acc > best_val_acc if val_acc is not None else False
            if is_best:
                best_val_acc = val_acc
                best_epoch = epoch + 1
                self.logger.log_message(f"æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}% (Epoch {best_epoch})")
            
            if epoch % self.args.save_freq == 0 or is_best or epoch == self.args.epochs - 1:
                self.save_checkpoint(epoch, train_loss, val_loss, val_acc, is_best)
                
                # # å¢å¼ºåŠŸèƒ½ï¼šåœ¨æœ€ä½³æ¨¡å‹æ—¶ç”Ÿæˆæ··æ·†çŸ©é˜µ
                # if is_best and self.enhanced_trainer and self.args.save_confusion_matrix:
                #     try:
                #         print("   ğŸ“Š ç”Ÿæˆæœ€ä½³æ¨¡å‹æ··æ·†çŸ©é˜µ...")
                #         self.logger.log_message("ç”Ÿæˆæœ€ä½³æ¨¡å‹æ··æ·†çŸ©é˜µ")
                #         self._generate_confusion_matrix(epoch + 1)
                #     except Exception as e:
                #         print(f"   âš ï¸ æ··æ·†çŸ©é˜µç”Ÿæˆå¤±è´¥: {e}")
                #         self.logger.log_message(f"æ··æ·†çŸ©é˜µç”Ÿæˆå¤±è´¥: {e}", "WARNING")
        
        # è®¡ç®—æ€»è®­ç»ƒæ—¶é—´
        total_time = (time.time() - self.start_time) / 60  # è½¬æ¢ä¸ºåˆ†é’Ÿ
        
        print(f"\nğŸ‰ Training completed! Best validation accuracy: {best_val_acc:.2f}% (Epoch {best_epoch})")
        print(f"â±ï¸ Total training time: {total_time:.2f} minutes")
        
        # ğŸ”§ ç»¼åˆéªŒè¯ç»“æœæŠ¥å‘Š
        if self.cv_results:
            print(f"\nğŸ“Š ç»¼åˆéªŒè¯ç»“æœåˆ†æ:")
            print(f"{'='*60}")
            
            # æœ€ä½³äº¤å‰éªŒè¯ç»“æœ
            best_cv = max(self.cv_results, key=lambda x: x['avg_acc'])
            print(f"ğŸ† æœ€ä½³äº¤å‰éªŒè¯ç»“æœ (Epoch {best_cv['epoch']}):")
            print(f"   å¹³å‡å‡†ç¡®ç‡: {best_cv['avg_acc']:.2f}% Â± {best_cv['std_acc']:.2f}%")
            print(f"   å¹³å‡æŸå¤±: {best_cv['avg_loss']:.4f} Â± {best_cv['std_loss']:.4f}")
            
            # æœ€ç»ˆäº¤å‰éªŒè¯ç»“æœ
            final_cv = self.cv_results[-1]
            print(f"ğŸ“ˆ æœ€ç»ˆäº¤å‰éªŒè¯ç»“æœ (Epoch {final_cv['epoch']}):")
            print(f"   å¹³å‡å‡†ç¡®ç‡: {final_cv['avg_acc']:.2f}% Â± {final_cv['std_acc']:.2f}%")
            print(f"   å¹³å‡æŸå¤±: {final_cv['avg_loss']:.4f} Â± {final_cv['std_loss']:.4f}")
            
            # éªŒè¯ç¨³å®šæ€§åˆ†æ
            cv_accs = [cv['avg_acc'] for cv in self.cv_results]
            import numpy as np
            trend = "ç¨³å®š" if np.std(cv_accs) < 5.0 else "ä¸ç¨³å®š"
            print(f"ğŸ¯ éªŒè¯ç¨³å®šæ€§: {trend} (æ ‡å‡†å·®: {np.std(cv_accs):.2f}%)")
            
            # å¯¹æ¯”ä¼ ç»ŸéªŒè¯vsäº¤å‰éªŒè¯
            print(f"\nâš–ï¸  éªŒè¯æ–¹æ³•å¯¹æ¯”:")
            print(f"   ä¼ ç»ŸéªŒè¯æœ€ä½³å‡†ç¡®ç‡: {best_val_acc:.2f}%")
            print(f"   äº¤å‰éªŒè¯æœ€ä½³å‡†ç¡®ç‡: {best_cv['avg_acc']:.2f}% Â± {best_cv['std_acc']:.2f}%")
            
            accuracy_diff = abs(best_val_acc - best_cv['avg_acc'])
            if accuracy_diff > 10.0:
                print(f"   âš ï¸  å·®å¼‚è¾ƒå¤§({accuracy_diff:.1f}%)ï¼Œå¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆé£é™©")
            elif accuracy_diff > 5.0:
                print(f"   âš ï¸  å­˜åœ¨ä¸€å®šå·®å¼‚({accuracy_diff:.1f}%)ï¼Œå»ºè®®å…³æ³¨")
            else:
                print(f"   âœ… ç»“æœä¸€è‡´({accuracy_diff:.1f}%)ï¼ŒéªŒè¯å¯é ")
            
            # è®°å½•ç»¼åˆåˆ†æåˆ°æ—¥å¿—
            self.logger.log_message("="*60)
            self.logger.log_message("ç»¼åˆéªŒè¯ç»“æœåˆ†æ")
            self.logger.log_message(f"æœ€ä½³äº¤å‰éªŒè¯: {best_cv['avg_acc']:.2f}% Â± {best_cv['std_acc']:.2f}% (Epoch {best_cv['epoch']})")
            self.logger.log_message(f"æœ€ç»ˆäº¤å‰éªŒè¯: {final_cv['avg_acc']:.2f}% Â± {final_cv['std_acc']:.2f}% (Epoch {final_cv['epoch']})")
            self.logger.log_message(f"éªŒè¯ç¨³å®šæ€§: {trend}")
            self.logger.log_message(f"ä¼ ç»ŸéªŒè¯ vs äº¤å‰éªŒè¯å·®å¼‚: {accuracy_diff:.1f}%")
        else:
            print(f"\nğŸ“Š ä»…ä½¿ç”¨äº†ä¼ ç»ŸéªŒè¯æ–¹æ³•")
            if hasattr(self, 'dynamic_split_interval') and self.dynamic_split_interval > 0:
                resplit_count = self.args.epochs // self.dynamic_split_interval
                print(f"ğŸ”„ åŠ¨æ€éªŒè¯é›†é‡æ–°åˆ†å‰²æ¬¡æ•°: {resplit_count}")
                self.logger.log_message(f"åŠ¨æ€éªŒè¯é›†é‡æ–°åˆ†å‰²æ¬¡æ•°: {resplit_count}")
        
        # æœ€ç»ˆå¯è§†åŒ–
        if self.enhanced_trainer:
            if self.args.save_training_curves:
                try:
                    print("ğŸ“ˆ ç”Ÿæˆæœ€ç»ˆè®­ç»ƒæ›²çº¿...")
                    self.enhanced_trainer.visualize_training_progress(
                        self.train_losses, self.train_accs, 
                        self.val_losses if self.val_losses else None,
                        self.val_accs if self.val_accs else None
                    )
                    self.logger.log_message("ç”Ÿæˆæœ€ç»ˆè®­ç»ƒæ›²çº¿")
                except Exception as e:
                    print(f"âš ï¸ æœ€ç»ˆè®­ç»ƒæ›²çº¿ç”Ÿæˆå¤±è´¥: {e}")
                    self.logger.log_message(f"æœ€ç»ˆè®­ç»ƒæ›²çº¿ç”Ÿæˆå¤±è´¥: {e}", "WARNING")
        
        # è®°å½•è®­ç»ƒç»“æŸ
        self.logger.log_training_end(best_epoch, best_val_acc, total_time)
        self.logger.close()
        
        print(f"ğŸ“ è¯¦ç»†æ—¥å¿—å·²ä¿å­˜åˆ°: {self.logger.get_experiment_dir()}")

def main():
    parser = argparse.ArgumentParser(description='CPR-TaG-Netè¡€ç®¡åˆ†ç±»æ¨¡å‹è®­ç»ƒ')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--data_dir', type=str, default='/home/lihe/classify/lungmap/data/processed',
                       help='é¢„å¤„ç†æ•°æ®ç›®å½•')
    parser.add_argument('--max_nodes', type=int, default=1000, 
                       help='æœ€å¤§èŠ‚ç‚¹æ•°é™åˆ¶ï¼ˆè¿‡æ»¤å¤§æ¡ˆä¾‹ï¼‰')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--node_batch_size', type=int, default=500, help='èŠ‚ç‚¹æ‰¹å¤§å°')  # 24GBæ˜¾å­˜å¯ä»¥æ›´å¤§
    parser.add_argument('--learning_rate', type=float, default=0.001, help='å­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=1e-4, help='æƒé‡è¡°å‡')
    parser.add_argument('--step_size', type=int, default=20, help='å­¦ä¹ ç‡è¡°å‡æ­¥é•¿')
    parser.add_argument('--gamma', type=float, default=0.5, help='å­¦ä¹ ç‡è¡°å‡å› å­')
    
    # 24GBæ˜¾å­˜ä¼˜åŒ–å‚æ•°
    parser.add_argument('--max_nodes_per_case', type=int, default=8000, 
                       help='å•æ¡ˆä¾‹æœ€å¤§èŠ‚ç‚¹æ•°ï¼ˆ24GBæ˜¾å­˜ä¼˜åŒ–ï¼‰')
    parser.add_argument('--enable_large_cases', action='store_true', 
                       help='å¯ç”¨å¤§æ¡ˆä¾‹è®­ç»ƒï¼ˆéœ€è¦24GBæ˜¾å­˜ï¼‰')
    
    # ç³»ç»Ÿå‚æ•°
    parser.add_argument('--checkpoint_dir', type=str, default='/home/lihe/classify/lungmap/outputs/checkpoints',
                       help='æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•')
    parser.add_argument('--log_dir', type=str, default='/home/lihe/classify/lungmap/outputs/logs',
                       help='æ—¥å¿—ä¿å­˜ç›®å½•')
    parser.add_argument('--save_freq', type=int, default=5, help='æ£€æŸ¥ç‚¹ä¿å­˜é¢‘ç‡')
    
    # å¢å¼ºåŠŸèƒ½å‚æ•°
    parser.add_argument('--enable_graph_completion', action='store_true',
                       help='å¯ç”¨å›¾å½¢è¡¥å…¨åŠŸèƒ½')
    parser.add_argument('--enable_visualization', action='store_true',
                       help='å¯ç”¨è®­ç»ƒå¯è§†åŒ–åŠŸèƒ½')
    parser.add_argument('--save_confusion_matrix', action='store_true',
                       help='ä¿å­˜æ··æ·†çŸ©é˜µ')
    parser.add_argument('--save_training_curves', action='store_true',
                       help='ä¿å­˜è®­ç»ƒæ›²çº¿')
    parser.add_argument('--visualization_dir', type=str, default='/home/lihe/classify/lungmap/outputs/visualizations',
                       help='å¯è§†åŒ–ç»“æœä¿å­˜ç›®å½•')
    
    # ğŸ”§ è¡€ç®¡æ„ŸçŸ¥è®­ç»ƒå‚æ•°
    parser.add_argument('--enable_vessel_aware', action='store_true', default=True,
                       help='å¯ç”¨è¡€ç®¡æ„ŸçŸ¥è®­ç»ƒï¼ˆæ¨èï¼‰')
    parser.add_argument('--vessel_consistency_weight', type=float, default=0.1,
                       help='è¡€ç®¡ä¸€è‡´æ€§æŸå¤±æƒé‡')
    parser.add_argument('--spatial_consistency_weight', type=float, default=0.05,
                       help='ç©ºé—´è¿ç»­æ€§æŸå¤±æƒé‡')
    
    # ğŸ”§ éªŒè¯æ”¹è¿›å‚æ•°
    parser.add_argument('--dynamic_split_interval', type=int, default=10,
                       help='åŠ¨æ€é‡æ–°åˆ†å‰²æ•°æ®çš„é—´éš”(epoch)ï¼Œ0è¡¨ç¤ºç¦ç”¨')
    parser.add_argument('--enable_cross_validation', action='store_true',
                       help='å¯ç”¨K-foldäº¤å‰éªŒè¯')
    parser.add_argument('--cv_folds', type=int, default=5,
                       help='äº¤å‰éªŒè¯çš„foldæ•°é‡')
    parser.add_argument('--enable_leave_one_out', action='store_true',
                       help='å¯ç”¨ç•™ä¸€æ³•éªŒè¯(ä»…é€‚ç”¨äºå°æ•°æ®é›†)')
    
    args = parser.parse_args()
    
    # æ‰“å°é…ç½®
    print("ğŸ”§ CPR-TaG-Net Training Configuration:")
    print("ğŸ§  è¡€ç®¡æ„ŸçŸ¥è®­ç»ƒæ”¹è¿›ç‰ˆ - å……åˆ†åˆ©ç”¨è¡€ç®¡è¿æ¥å‰ç½®ä¿¡æ¯")
    print(f"  Data directory: {args.data_dir}")
    if args.enable_large_cases:
        print(f"  24GBæ˜¾å­˜æ¨¡å¼: å¯ç”¨å¤§æ¡ˆä¾‹è®­ç»ƒ")
        print(f"  Max nodes per case: {args.max_nodes_per_case}")
    else:
        print(f"  ä¿å®ˆæ¨¡å¼: Max nodes per case: {args.max_nodes}")
    print(f"  Epochs: {args.epochs}")
    print(f"  Node batch size: {args.node_batch_size}")
    print(f"  Learning rate: {args.learning_rate}")
    print(f"  Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
    
    # ğŸ”§ è¡€ç®¡æ„ŸçŸ¥è®­ç»ƒé…ç½®
    if args.enable_vessel_aware:
        print(f"  ğŸ©¸ è¡€ç®¡æ„ŸçŸ¥è®­ç»ƒ: å¯ç”¨")
        print(f"    - è¡€ç®¡å±‚æ¬¡é¡ºåºè®­ç»ƒ")
        print(f"    - è¡€ç®¡é—´è¿æ¥ä¿æŒ")
        print(f"    - è¡€ç®¡å…ˆéªŒä¿¡æ¯æ³¨å…¥")
        print(f"    - å±‚æ¬¡åŒ–æŸå¤±å‡½æ•°")
        print(f"    - è¡€ç®¡ä¸€è‡´æ€§æƒé‡: {args.vessel_consistency_weight}")
        print(f"    - ç©ºé—´è¿ç»­æ€§æƒé‡: {args.spatial_consistency_weight}")
    else:
        print(f"  âš ï¸  è¡€ç®¡æ„ŸçŸ¥è®­ç»ƒ: ç¦ç”¨ï¼ˆä¸æ¨èï¼‰")
    
    # ğŸ”§ éªŒè¯æ”¹è¿›åŠŸèƒ½é…ç½®
    validation_features = []
    if args.dynamic_split_interval > 0:
        validation_features.append(f"åŠ¨æ€éªŒè¯é›†(æ¯{args.dynamic_split_interval}epoch)")
    if args.enable_cross_validation:
        validation_features.append(f"{args.cv_folds}-foldäº¤å‰éªŒè¯")
    if args.enable_leave_one_out:
        validation_features.append("ç•™ä¸€æ³•éªŒè¯")
    
    if validation_features:
        print(f"  ğŸ”¬ éªŒè¯æ”¹è¿›åŠŸèƒ½: {', '.join(validation_features)}")
        print(f"    - ç»Ÿä¸€æŸå¤±å‡½æ•°: éªŒè¯æ—¶ä½¿ç”¨å±‚çº§æŸå¤±")
    else:
        print(f"  ğŸ”¬ éªŒè¯æ”¹è¿›åŠŸèƒ½: ä»…ç»Ÿä¸€æŸå¤±å‡½æ•°")
    
    # å¢å¼ºåŠŸèƒ½é…ç½®
    enhanced_features = []
    if args.enable_graph_completion:
        enhanced_features.append("å›¾å½¢è¡¥å…¨")
    if args.enable_visualization:
        enhanced_features.append("è®­ç»ƒå¯è§†åŒ–")
    if args.save_confusion_matrix:
        enhanced_features.append("æ··æ·†çŸ©é˜µ")
    if args.save_training_curves:
        enhanced_features.append("è®­ç»ƒæ›²çº¿")
    
    if enhanced_features:
        print(f"  ğŸ§  å¢å¼ºåŠŸèƒ½: {', '.join(enhanced_features)}")
        print(f"  ğŸ“Š å¯è§†åŒ–ç›®å½•: {args.visualization_dir}")
    else:
        print(f"  åŸºç¡€è®­ç»ƒæ¨¡å¼ï¼ˆæ— å¢å¼ºåŠŸèƒ½ï¼‰")
    
    # GPUæ˜¾å­˜æ£€æŸ¥
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"  GPU Memory: {gpu_memory:.1f}GB")
        if gpu_memory >= 20 and not args.enable_large_cases:
            print(f"  ğŸ’¡ æ£€æµ‹åˆ°å¤§æ˜¾å­˜GPUï¼Œå»ºè®®ä½¿ç”¨ --enable_large_cases è®­ç»ƒæ›´å¤šæ•°æ®")
    
    # å¼€å§‹è®­ç»ƒ
    trainer = VesselTrainer(args)
    trainer.train()

if __name__ == "__main__":
    main()
