#!/usr/bin/env python3
"""
è¡€ç®¡åˆ†ç±»æ¨¡å‹è®­ç»ƒå¯åŠ¨è„šæœ¬ - æ•´åˆæ•°æ®åŠ è½½å’ŒCPR-TaG-Netè®­ç»ƒ
"""

import os
import sys
import argparse
import time
from tqdm import tqdm
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter

# æ·»åŠ æ¨¡å—è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src', 'training'))
sys.path.append(os.path.join(os.path.dirname(__file__), 'src', 'models', 'CPR_TaG_Net'))

from vessel_data_loader import load_processed_data, create_data_splits, create_dataloaders
from models.cpr_tagnet import CPRTaGNet

class VesselTrainer:
    """è¡€ç®¡åˆ†ç±»è®­ç»ƒå™¨"""
    
    def __init__(self, args):
        self.args = args
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ Using device: {self.device}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(args.checkpoint_dir, exist_ok=True)
        os.makedirs(args.log_dir, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self.writer = SummaryWriter(args.log_dir)
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._setup_model()
        
        # åŠ è½½æ•°æ®
        self._setup_data()
        
        # è®¾ç½®è®­ç»ƒå‚æ•°
        self._setup_training()
    
    def _setup_model(self):
        """åˆå§‹åŒ–CPR-TaG-Netæ¨¡å‹"""
        print("ğŸ”§ Setting up CPR-TaG-Net model...")
        
        # CPR-TaG-Net é…ç½®å‚æ•°
        model_config = {
            'num_classes': 15,  # æ ¹æ®æ‚¨çš„æ•°æ®ï¼Œæœ‰15ä¸ªç±»åˆ«ï¼ˆ0-14ï¼‰
            'node_feature_dim': 54,  # èŠ‚ç‚¹ç‰¹å¾ç»´åº¦
            'image_channels': 1,  # å›¾åƒé€šé“æ•°
        }
        
        self.model = CPRTaGNet(**model_config).to(self.device)
        print(f"âœ… CPR-TaG-Net initialized with {sum(p.numel() for p in self.model.parameters()):,} parameters")
        
    def _setup_data(self):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨ - 24GBæ˜¾å­˜ä¼˜åŒ–ç‰ˆæœ¬"""
        print("ğŸ“Š Loading and preparing data...")
        
        # åŠ è½½é¢„å¤„ç†æ•°æ®
        data_list = load_processed_data(self.args.data_dir)
        print(f"âœ… Loaded {len(data_list)} cases")
        
        # æ ¹æ®æ˜¾å­˜æƒ…å†µç­›é€‰æ•°æ®
        if self.args.enable_large_cases:
            # 24GBæ˜¾å­˜ï¼šä½¿ç”¨å¤§éƒ¨åˆ†æ•°æ®
            filtered_data = []
            for data in data_list:
                nodes = len(data['node_features'])
                if nodes < self.args.max_nodes_per_case:
                    filtered_data.append(data)
                    print(f"  âœ… {data['case_id']}: {nodes} nodes")
                else:
                    print(f"  âš ï¸  {data['case_id']}: {nodes} nodes (è·³è¿‡ï¼Œè¶…è¿‡{self.args.max_nodes_per_case})")
            
            print(f"ğŸ“Š Using {len(filtered_data)} cases (24GBæ˜¾å­˜ä¼˜åŒ–)")
        else:
            # ä¿å®ˆæ¨¡å¼ï¼šåªä½¿ç”¨å°æ•°æ®é›†
            filtered_data = []
            for data in data_list:
                if len(data['node_features']) < self.args.max_nodes:
                    filtered_data.append(data)
                    print(f"  âœ… {data['case_id']}: {len(data['node_features'])} nodes")
            
            print(f"ğŸ“Š Selected {len(filtered_data)} small cases for training")
        
        if len(filtered_data) == 0:
            print("âŒ No suitable cases found, using first 3 cases")
            filtered_data = data_list[:3]
        
        # åˆ›å»ºæ•°æ®åˆ†å‰²
        train_data, val_data, test_data = create_data_splits(
            filtered_data, 
            train_ratio=0.7, 
            val_ratio=0.15, 
            test_ratio=0.15,
            random_state=42
        )
        
        # ä¿å­˜ä¸ºç±»å±æ€§
        self.train_data = train_data
        self.val_data = val_data
        self.test_data = test_data
        
        print(f"âœ… Data splits - Train: {len(self.train_data)}, Val: {len(self.val_data)}, Test: {len(self.test_data)}")
        
        # æ˜¾å­˜ä½¿ç”¨é¢„ä¼°
        total_train_nodes = sum(len(d['node_features']) for d in self.train_data)
        estimated_memory_gb = total_train_nodes * 0.25 / 1024  # æ¯èŠ‚ç‚¹çº¦0.25MB
        print(f"ğŸ’¾ Estimated training memory: {estimated_memory_gb:.1f}GB")
        
    def _setup_training(self):
        """è®¾ç½®è®­ç»ƒå‚æ•°"""
        print("ğŸ”§ Setting up training parameters...")
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss()
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=self.args.learning_rate,
            weight_decay=self.args.weight_decay
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.StepLR(
            self.optimizer,
            step_size=self.args.step_size,
            gamma=self.args.gamma
        )
        
        print("âœ… Training setup completed")
    
    def train_on_case(self, case_data, epoch, case_idx):
        """åœ¨å•ä¸ªcaseä¸Šè®­ç»ƒï¼ˆåˆ†æ‰¹å¤„ç†èŠ‚ç‚¹ï¼‰- é€‚é…CPR-TaG-Netï¼Œè‡ªé€‚åº”æ‰¹å¤§å°"""
        self.model.train()
        
        case_id = case_data['case_id']
        
        # å‡†å¤‡æ•°æ®
        node_features = torch.FloatTensor(case_data['node_features']).to(self.device)
        node_positions = torch.FloatTensor(case_data['node_positions']).to(self.device)
        edge_index = torch.LongTensor(case_data['edge_index']).to(self.device)
        image_cubes = torch.FloatTensor(case_data['image_cubes']).unsqueeze(1).to(self.device)  # [N, 1, 32, 32, 32]
        node_classes = torch.LongTensor(case_data['node_classes']).to(self.device)
        
        num_nodes = node_features.shape[0]
        
        # è‡ªé€‚åº”æ‰¹å¤§å°ï¼šæ ¹æ®èŠ‚ç‚¹æ•°é‡è°ƒæ•´
        if num_nodes > 5000:
            batch_size = min(200, num_nodes)  # å¤§æ¡ˆä¾‹ç”¨å°æ‰¹
        elif num_nodes > 2000:
            batch_size = min(300, num_nodes)  # ä¸­æ¡ˆä¾‹ç”¨ä¸­æ‰¹
        else:
            batch_size = min(self.args.node_batch_size, num_nodes)  # å°æ¡ˆä¾‹ç”¨å¤§æ‰¹
            
        num_batches = (num_nodes + batch_size - 1) // batch_size
        
        total_loss = 0.0
        total_correct = 0
        total_samples = 0
        successful_batches = 0
        
        # éšæœºæ‰“ä¹±èŠ‚ç‚¹é¡ºåº
        indices = torch.randperm(num_nodes, device=self.device)
        
        for batch_idx in range(num_batches):
            start_idx = batch_idx * batch_size
            end_idx = min((batch_idx + 1) * batch_size, num_nodes)
            batch_indices = indices[start_idx:end_idx]
            
            # ğŸ”§ æ”¹è¿›å†…å­˜ç®¡ç†ï¼šé¢„å…ˆæ¸…ç†
            if batch_idx > 0:
                torch.cuda.empty_cache()
            
            # æ‰¹æ•°æ®å‡†å¤‡ - æ·»åŠ é”™è¯¯æ£€æŸ¥
            try:
                batch_node_features = node_features[batch_indices]
                batch_node_positions = node_positions[batch_indices]
                batch_image_cubes = image_cubes[batch_indices]
                batch_node_classes = node_classes[batch_indices]
                
                # æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
                if batch_node_features.shape[0] == 0:
                    print(f"âš ï¸  Empty batch at {batch_idx}, skipping...")
                    continue
                    
                # æ£€æŸ¥ç‰¹å¾ç»´åº¦
                if batch_node_features.shape[1] not in [54, 64]:  # å…è®¸çš„ç‰¹å¾ç»´åº¦
                    print(f"âš ï¸  Unexpected feature dimension: {batch_node_features.shape}")
                    
            except Exception as e:
                print(f"âš ï¸  Error preparing batch {batch_idx}: {e}")
                continue
            
            # ä¸ºCPR-TaG-Netå‡†å¤‡è¾¹è¿æ¥ - è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œåªä¿ç•™æ‰¹å†…è¿æ¥
            batch_edge_index = self._prepare_batch_edges(edge_index, batch_indices, batch_size)
            
            try:
                # å‰å‘ä¼ æ’­
                self.optimizer.zero_grad()
                
                outputs = self.model(
                    batch_node_features, 
                    batch_node_positions, 
                    batch_edge_index, 
                    batch_image_cubes
                )
                
                loss = self.criterion(outputs, batch_node_classes)
                
                # åå‘ä¼ æ’­
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                self.optimizer.step()
                
                # ç»Ÿè®¡
                total_loss += loss.item()
                _, predicted = outputs.max(1)
                total_correct += predicted.eq(batch_node_classes).sum().item()
                total_samples += batch_node_classes.size(0)
                successful_batches += 1
                
                # è®°å½•åˆ°tensorboard
                if batch_idx % 5 == 0:
                    global_step = epoch * len(self.train_data) * 10 + case_idx * 10 + batch_idx
                    self.writer.add_scalar('Train/BatchLoss', loss.item(), global_step)
                
                # æ¸…ç†å†…å­˜
                del batch_node_features, batch_node_positions, batch_image_cubes, batch_node_classes, outputs
                del batch_edge_index
                torch.cuda.empty_cache()
                
            except RuntimeError as e:
                if "out of memory" in str(e):
                    # ğŸ”§ æ”¹è¿›çš„OOMå¤„ç†
                    torch.cuda.empty_cache()
                    if batch_size > 32:  # é™ä½æœ€å°æ‰¹å¤§å°é˜ˆå€¼
                        batch_size = max(32, batch_size // 2)
                        print(f"âš ï¸  {case_id} OOM, reducing batch size to {batch_size}")
                        # å¼ºåˆ¶å†…å­˜æ¸…ç†
                        if 'batch_node_features' in locals():
                            del batch_node_features, batch_node_positions, batch_image_cubes, batch_node_classes
                        if 'batch_edge_index' in locals():
                            del batch_edge_index
                        if 'outputs' in locals():
                            del outputs
                        torch.cuda.empty_cache()
                        continue
                    else:
                        print(f"âš ï¸  {case_id} batch {batch_idx} OOM (min batch size reached), skipping...")
                        # è·³è¿‡å½“å‰æ‰¹æ¬¡ï¼Œç»§ç»­ä¸‹ä¸€ä¸ª
                        torch.cuda.empty_cache()
                        continue
                elif "size of tensor" in str(e) or "mat1 and mat2 shapes cannot be multiplied" in str(e):
                    print(f"âš ï¸  {case_id} tensor dimension mismatch: {e}")
                    print(f"    Node features: {batch_node_features.shape if 'batch_node_features' in locals() else 'N/A'}")
                    print(f"    Image cubes: {batch_image_cubes.shape if 'batch_image_cubes' in locals() else 'N/A'}")
                    # æ¸…ç†å†…å­˜å¹¶ç»§ç»­
                    torch.cuda.empty_cache()
                    continue
                else:
                    # å…¶ä»–é”™è¯¯ï¼Œé‡æ–°æŠ›å‡º
                    print(f"âš ï¸  Unexpected error in {case_id}: {e}")
                    raise e
        
        # æ¸…ç†caseçº§åˆ«çš„æ•°æ®
        del node_features, node_positions, edge_index, image_cubes, node_classes
        torch.cuda.empty_cache()
        
        avg_loss = total_loss / successful_batches if successful_batches > 0 else 0.0
        accuracy = 100.0 * total_correct / total_samples if total_samples > 0 else 0.0
        
        return avg_loss, accuracy, total_samples
    
    def _prepare_batch_edges(self, edge_index, batch_indices, batch_size):
        """ä¸ºæ‰¹å¤„ç†å‡†å¤‡è¾¹è¿æ¥ - æ”¹è¿›ç‰ˆæœ¬"""
        device = edge_index.device
        
        # å¦‚æœæ²¡æœ‰è¾¹ï¼Œè¿”å›ç©ºè¾¹ç´¢å¼•
        if edge_index.shape[1] == 0:
            return torch.zeros((2, 0), dtype=torch.long, device=device)
        
        # åˆ›å»ºç´¢å¼•æ˜ å°„
        max_idx = max(edge_index.max().item(), batch_indices.max().item())
        old_to_new = torch.full((max_idx + 1,), -1, device=device)
        old_to_new[batch_indices] = torch.arange(len(batch_indices), device=device)
        
        # æ‰¾åˆ°æ‰¹å†…çš„è¾¹
        src_in_batch = torch.isin(edge_index[0], batch_indices)
        dst_in_batch = torch.isin(edge_index[1], batch_indices)
        edge_mask = src_in_batch & dst_in_batch
        
        if edge_mask.sum() == 0:
            # æ²¡æœ‰æ‰¹å†…è¾¹ï¼Œè¿”å›ç©ºè¾¹ç´¢å¼•
            return torch.zeros((2, 0), dtype=torch.long, device=device)
        
        # è·å–æ‰¹å†…è¾¹å¹¶é‡æ–°ç´¢å¼•
        batch_edges = edge_index[:, edge_mask]
        
        # å®‰å…¨åœ°é‡æ–°ç´¢å¼•
        try:
            batch_edges[0] = old_to_new[batch_edges[0]]
            batch_edges[1] = old_to_new[batch_edges[1]]
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ— æ•ˆçš„ç´¢å¼•
            if (batch_edges < 0).any() or (batch_edges >= len(batch_indices)).any():
                print(f"âš ï¸  Invalid edge indices detected, returning empty edge index")
                return torch.zeros((2, 0), dtype=torch.long, device=device)
            
            return batch_edges
            
        except Exception as e:
            print(f"âš ï¸  Error in edge preparation: {e}, returning empty edge index")
            return torch.zeros((2, 0), dtype=torch.long, device=device)

    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch - åŸºäºcaseçš„æ‰¹å¤„ç†"""
        print(f"\n{'='*50}")
        print(f"Epoch {epoch+1}/{self.args.epochs}")
        print(f"{'='*50}")
        
        epoch_loss = 0.0
        epoch_correct = 0
        epoch_samples = 0
        
        # éšæœºæ‰“ä¹±è®­ç»ƒæ•°æ®
        import random
        train_indices = list(range(len(self.train_data)))
        random.shuffle(train_indices)
        
        pbar = tqdm(train_indices, desc='Training Cases')
        for i, case_idx in enumerate(pbar):
            case_data = self.train_data[case_idx]
            case_id = case_data['case_id']
            num_nodes = len(case_data['node_features'])
            
            try:
                loss, acc, samples = self.train_on_case(case_data, epoch, i)
                
                epoch_loss += loss * samples
                epoch_correct += acc * samples / 100.0
                epoch_samples += samples
                
                current_acc = 100.0 * epoch_correct / epoch_samples if epoch_samples > 0 else 0.0
                
                pbar.set_postfix({
                    'Case': case_id,
                    'Nodes': num_nodes,
                    'Loss': f'{loss:.3f}',
                    'Acc': f'{current_acc:.1f}%',
                    'GPU': f'{torch.cuda.memory_allocated()/1024**3:.1f}GB'
                })
                
            except Exception as e:
                print(f"âŒ Error training on {case_id}: {e}")
                continue
        
        avg_loss = epoch_loss / epoch_samples if epoch_samples > 0 else 0.0
        avg_acc = 100.0 * epoch_correct / epoch_samples if epoch_samples > 0 else 0.0
        
        # è®°å½•åˆ°tensorboard
        self.writer.add_scalar('Train/EpochLoss', avg_loss, epoch)
        self.writer.add_scalar('Train/EpochAccuracy', avg_acc, epoch)
        
        return avg_loss, avg_acc
    
    def validate_on_case(self, case_data):
        """åœ¨å•ä¸ªcaseä¸ŠéªŒè¯ - é€‚é…CPR-TaG-Netï¼Œè‡ªé€‚åº”æ‰¹å¤§å°"""
        self.model.eval()
        
        case_id = case_data['case_id']
        
        with torch.no_grad():
            # å‡†å¤‡æ•°æ®
            node_features = torch.FloatTensor(case_data['node_features']).to(self.device)
            node_positions = torch.FloatTensor(case_data['node_positions']).to(self.device)
            edge_index = torch.LongTensor(case_data['edge_index']).to(self.device)
            image_cubes = torch.FloatTensor(case_data['image_cubes']).unsqueeze(1).to(self.device)
            node_classes = torch.LongTensor(case_data['node_classes']).to(self.device)
            
            num_nodes = node_features.shape[0]
            
            # è‡ªé€‚åº”æ‰¹å¤§å°
            if num_nodes > 5000:
                batch_size = min(200, num_nodes)
            elif num_nodes > 2000:
                batch_size = min(300, num_nodes)
            else:
                batch_size = min(self.args.node_batch_size, num_nodes)
                
            num_batches = (num_nodes + batch_size - 1) // batch_size
            
            total_loss = 0.0
            total_correct = 0
            total_samples = 0
            successful_batches = 0
            
            for batch_idx in range(num_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, num_nodes)
                
                # æ‰¹æ•°æ®
                batch_indices = torch.arange(start_idx, end_idx, device=self.device)
                batch_node_features = node_features[batch_indices]
                batch_node_positions = node_positions[batch_indices]
                batch_image_cubes = image_cubes[batch_indices]
                batch_node_classes = node_classes[batch_indices]
                
                # ä¸ºCPR-TaG-Netå‡†å¤‡è¾¹è¿æ¥
                batch_edge_index = self._prepare_batch_edges(edge_index, batch_indices, batch_size)
                
                try:
                    outputs = self.model(
                        batch_node_features, 
                        batch_node_positions, 
                        batch_edge_index, 
                        batch_image_cubes
                    )
                    
                    loss = self.criterion(outputs, batch_node_classes)
                    
                    # ç»Ÿè®¡
                    total_loss += loss.item()
                    _, predicted = outputs.max(1)
                    total_correct += predicted.eq(batch_node_classes).sum().item()
                    total_samples += batch_node_classes.size(0)
                    successful_batches += 1
                    
                    # æ¸…ç†å†…å­˜
                    del batch_node_features, batch_node_positions, batch_image_cubes, batch_node_classes, outputs
                    del batch_edge_index
                    torch.cuda.empty_cache()
                    
                except RuntimeError as e:
                    if "out of memory" in str(e):
                        torch.cuda.empty_cache()
                        if batch_size > 50:
                            batch_size = batch_size // 2
                            print(f"âš ï¸  Validation {case_id} OOM, reducing batch size to {batch_size}")
                            continue
                        else:
                            print(f"âš ï¸  Validation {case_id} batch {batch_idx} OOM, skipping...")
                            continue
                    elif "size of tensor" in str(e):
                        print(f"âš ï¸  Validation {case_id} tensor dimension mismatch: {e}")
                        continue
                    else:
                        raise e
            
            # æ¸…ç†caseçº§åˆ«çš„æ•°æ®
            del node_features, node_positions, edge_index, image_cubes, node_classes
            torch.cuda.empty_cache()
            
            avg_loss = total_loss / successful_batches if successful_batches > 0 else 0.0
            accuracy = 100.0 * total_correct / total_samples if total_samples > 0 else 0.0
            
            return avg_loss, accuracy, total_samples

    def validate(self, epoch):
        """éªŒè¯ä¸€ä¸ªepoch"""
        if not self.val_data:
            return 0.0, 0.0
        
        print("\nğŸ” Validating...")
        
        epoch_loss = 0.0
        epoch_correct = 0
        epoch_samples = 0
        
        pbar = tqdm(self.val_data, desc='Validation Cases')
        for case_data in pbar:
            case_id = case_data['case_id']
            num_nodes = len(case_data['node_features'])
            
            try:
                loss, acc, samples = self.validate_on_case(case_data)
                
                epoch_loss += loss * samples
                epoch_correct += acc * samples / 100.0
                epoch_samples += samples
                
                current_acc = 100.0 * epoch_correct / epoch_samples if epoch_samples > 0 else 0.0
                
                pbar.set_postfix({
                    'Case': case_id,
                    'Nodes': num_nodes,
                    'Loss': f'{loss:.3f}',
                    'Acc': f'{current_acc:.1f}%'
                })
                
            except Exception as e:
                print(f"âŒ Error validating on {case_id}: {e}")
                continue
        
        avg_loss = epoch_loss / epoch_samples if epoch_samples > 0 else 0.0
        avg_acc = 100.0 * epoch_correct / epoch_samples if epoch_samples > 0 else 0.0
        
        # è®°å½•åˆ°tensorboard
        self.writer.add_scalar('Val/EpochLoss', avg_loss, epoch)
        self.writer.add_scalar('Val/EpochAccuracy', avg_acc, epoch)
        
        return avg_loss, avg_acc
    
    def save_checkpoint(self, epoch, train_loss, val_loss, val_acc, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'train_loss': train_loss,
            'val_loss': val_loss,
            'val_accuracy': val_acc,
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        checkpoint_path = os.path.join(self.args.checkpoint_dir, 'latest.pth')
        torch.save(checkpoint, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = os.path.join(self.args.checkpoint_dir, 'best.pth')
            torch.save(checkpoint, best_path)
            print(f"ğŸ’¾ Best model saved at epoch {epoch+1}")
    
    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        print("ğŸš€ Starting CPR-TaG-Net training...")
        
        best_val_acc = 0.0
        
        for epoch in range(self.args.epochs):
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(epoch)
            
            # éªŒè¯
            val_loss, val_acc = self.validate(epoch)
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            
            # æ‰“å°ç»“æœ
            print(f"\nğŸ“Š Epoch {epoch+1} Results:")
            print(f"  Training   - Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%")
            if self.val_data:
                print(f"  Validation - Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%")
            print(f"  Learning Rate: {self.optimizer.param_groups[0]['lr']:.6f}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            is_best = val_acc > best_val_acc
            if is_best:
                best_val_acc = val_acc
            
            if epoch % self.args.save_freq == 0 or is_best or epoch == self.args.epochs - 1:
                self.save_checkpoint(epoch, train_loss, val_loss, val_acc, is_best)
        
        print(f"\nğŸ‰ Training completed! Best validation accuracy: {best_val_acc:.2f}%")
        self.writer.close()

def main():
    parser = argparse.ArgumentParser(description='CPR-TaG-Netè¡€ç®¡åˆ†ç±»æ¨¡å‹è®­ç»ƒ')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--data_dir', type=str, default='/home/lihe/classify/lungmap/data/processed',
                       help='é¢„å¤„ç†æ•°æ®ç›®å½•')
    parser.add_argument('--max_nodes', type=int, default=1000, 
                       help='æœ€å¤§èŠ‚ç‚¹æ•°é™åˆ¶ï¼ˆè¿‡æ»¤å¤§æ¡ˆä¾‹ï¼‰')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--node_batch_size', type=int, default=500, help='èŠ‚ç‚¹æ‰¹å¤§å°')  # 24GBæ˜¾å­˜å¯ä»¥æ›´å¤§
    parser.add_argument('--learning_rate', type=float, default=0.001, help='å­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=1e-4, help='æƒé‡è¡°å‡')
    parser.add_argument('--step_size', type=int, default=20, help='å­¦ä¹ ç‡è¡°å‡æ­¥é•¿')
    parser.add_argument('--gamma', type=float, default=0.5, help='å­¦ä¹ ç‡è¡°å‡å› å­')
    
    # 24GBæ˜¾å­˜ä¼˜åŒ–å‚æ•°
    parser.add_argument('--max_nodes_per_case', type=int, default=8000, 
                       help='å•æ¡ˆä¾‹æœ€å¤§èŠ‚ç‚¹æ•°ï¼ˆ24GBæ˜¾å­˜ä¼˜åŒ–ï¼‰')
    parser.add_argument('--enable_large_cases', action='store_true', 
                       help='å¯ç”¨å¤§æ¡ˆä¾‹è®­ç»ƒï¼ˆéœ€è¦24GBæ˜¾å­˜ï¼‰')
    
    # ç³»ç»Ÿå‚æ•°
    parser.add_argument('--checkpoint_dir', type=str, default='/home/lihe/classify/lungmap/outputs/checkpoints',
                       help='æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•')
    parser.add_argument('--log_dir', type=str, default='/home/lihe/classify/lungmap/outputs/logs',
                       help='æ—¥å¿—ä¿å­˜ç›®å½•')
    parser.add_argument('--save_freq', type=int, default=5, help='æ£€æŸ¥ç‚¹ä¿å­˜é¢‘ç‡')
    
    args = parser.parse_args()
    
    # æ‰“å°é…ç½®
    print("ğŸ”§ CPR-TaG-Net Training Configuration:")
    print(f"  Data directory: {args.data_dir}")
    if args.enable_large_cases:
        print(f"  24GBæ˜¾å­˜æ¨¡å¼: å¯ç”¨å¤§æ¡ˆä¾‹è®­ç»ƒ")
        print(f"  Max nodes per case: {args.max_nodes_per_case}")
    else:
        print(f"  ä¿å®ˆæ¨¡å¼: Max nodes per case: {args.max_nodes}")
    print(f"  Epochs: {args.epochs}")
    print(f"  Node batch size: {args.node_batch_size}")
    print(f"  Learning rate: {args.learning_rate}")
    print(f"  Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
    
    # GPUæ˜¾å­˜æ£€æŸ¥
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"  GPU Memory: {gpu_memory:.1f}GB")
        if gpu_memory >= 20 and not args.enable_large_cases:
            print(f"  ğŸ’¡ æ£€æµ‹åˆ°å¤§æ˜¾å­˜GPUï¼Œå»ºè®®ä½¿ç”¨ --enable_large_cases è®­ç»ƒæ›´å¤šæ•°æ®")
    
    # å¼€å§‹è®­ç»ƒ
    trainer = VesselTrainer(args)
    trainer.train()

if __name__ == "__main__":
    main()
