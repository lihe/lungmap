#!/usr/bin/env python3
"""
è®­ç»ƒç»“æœçœŸå®æ€§éªŒè¯è„šæœ¬
åˆ†æ100%å‡†ç¡®ç‡æ˜¯å¦ä¸ºçœŸå®æ€§èƒ½è¿˜æ˜¯å­˜åœ¨é—®é¢˜
"""

import os
import sys
import json
import numpy as np
import torch
from collections import Counter, defaultdict
from glob import glob
import matplotlib.pyplot as plt
import seaborn as sns

# æ·»åŠ æ¨¡å—è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src', 'training'))

class TrainingResultAnalyzer:
    """è®­ç»ƒç»“æœåˆ†æå™¨"""
    
    def __init__(self, data_dir, log_dir):
        self.data_dir = data_dir
        self.log_dir = log_dir
        self.vessel_hierarchy = {
            'MPA': {'level': 0, 'expected_class': 7},
            'LPA': {'level': 1, 'expected_class': 2},
            'RPA': {'level': 1, 'expected_class': 11},
            'Lupper': {'level': 2, 'expected_class': 6},
            'Rupper': {'level': 2, 'expected_class': 14},
            'L1+2': {'level': 2, 'expected_class': 0},
            'R1+2': {'level': 2, 'expected_class': 8},
            'L1+3': {'level': 2, 'expected_class': 1},
            'R1+3': {'level': 2, 'expected_class': 9},
            'Linternal': {'level': 2, 'expected_class': 4},
            'Rinternal': {'level': 2, 'expected_class': 12},
            'Lmedium': {'level': 2, 'expected_class': 5},
            'Rmedium': {'level': 2, 'expected_class': 13},
            'Ldown': {'level': 2, 'expected_class': 3},
            'RDown': {'level': 2, 'expected_class': 10}
        }
        
    def analyze_data_scale(self):
        """åˆ†ææ•°æ®è§„æ¨¡"""
        print("ğŸ” åˆ†æ1: æ•°æ®è§„æ¨¡æ£€æŸ¥")
        print("="*60)
        
        # è·å–æ‰€æœ‰æ•°æ®æ–‡ä»¶
        data_files = glob(os.path.join(self.data_dir, "*_processed.npz"))
        print(f"ğŸ“Š æ•°æ®æ–‡ä»¶æ•°é‡: {len(data_files)}")
        
        if len(data_files) == 0:
            print("âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼")
            return {}
        
        # åˆ†ææ¯ä¸ªæ–‡ä»¶
        total_nodes = 0
        total_edges = 0
        total_vessels = 0
        case_info = {}
        all_node_classes = []
        vessel_types = set()
        
        for file_path in sorted(data_files):
            case_id = os.path.basename(file_path).replace('_processed.npz', '')
            
            try:
                data = np.load(file_path, allow_pickle=True)
                
                # åŸºæœ¬ä¿¡æ¯
                num_nodes = len(data['node_features'])
                num_edges = data['edge_index'].shape[1] if 'edge_index' in data else 0
                vessel_ranges = data['vessel_node_ranges'].item() if 'vessel_node_ranges' in data else {}
                node_classes = data['node_classes'] if 'node_classes' in data else []
                
                case_info[case_id] = {
                    'nodes': num_nodes,
                    'edges': num_edges,
                    'vessels': len(vessel_ranges),
                    'vessel_types': list(vessel_ranges.keys()),
                    'node_classes': node_classes
                }
                
                total_nodes += num_nodes
                total_edges += num_edges
                total_vessels += len(vessel_ranges)
                all_node_classes.extend(node_classes)
                vessel_types.update(vessel_ranges.keys())
                
                print(f"  ğŸ“ {case_id}: {num_nodes}èŠ‚ç‚¹, {num_edges}è¾¹, {len(vessel_ranges)}è¡€ç®¡")
                
            except Exception as e:
                print(f"  âŒ æ— æ³•åŠ è½½ {case_id}: {e}")
        
        print(f"\nğŸ“Š æ€»è®¡:")
        print(f"  æ¡ˆä¾‹æ•°: {len(case_info)}")
        print(f"  æ€»èŠ‚ç‚¹æ•°: {total_nodes:,}")
        print(f"  æ€»è¾¹æ•°: {total_edges:,}")
        print(f"  æ€»è¡€ç®¡æ•°: {total_vessels}")
        print(f"  è¡€ç®¡ç±»å‹: {len(vessel_types)} ({sorted(vessel_types)})")
        
        # åˆ†æç±»åˆ«åˆ†å¸ƒ
        class_distribution = Counter(all_node_classes)
        print(f"\nğŸ“Š èŠ‚ç‚¹ç±»åˆ«åˆ†å¸ƒ:")
        for class_id in sorted(class_distribution.keys()):
            count = class_distribution[class_id]
            percentage = 100 * count / len(all_node_classes)
            print(f"  ç±»åˆ« {class_id}: {count:,} èŠ‚ç‚¹ ({percentage:.1f}%)")
        
        # æ£€æŸ¥ç±»åˆ«ä¸å¹³è¡¡
        if class_distribution:
            max_count = max(class_distribution.values())
            min_count = min(class_distribution.values())
            imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')
            print(f"\nâš–ï¸  ç±»åˆ«ä¸å¹³è¡¡æ¯”: {imbalance_ratio:.2f}")
            
            if imbalance_ratio > 10:
                print("âš ï¸  ä¸¥é‡ç±»åˆ«ä¸å¹³è¡¡ï¼Œå¯èƒ½å½±å“è®­ç»ƒç»“æœ")
            elif imbalance_ratio > 3:
                print("âš ï¸  å­˜åœ¨ç±»åˆ«ä¸å¹³è¡¡")
            else:
                print("âœ… ç±»åˆ«åˆ†å¸ƒç›¸å¯¹å‡è¡¡")
        
        return {
            'case_info': case_info,
            'total_stats': {
                'cases': len(case_info),
                'nodes': total_nodes,
                'edges': total_edges,
                'vessels': total_vessels
            },
            'class_distribution': dict(class_distribution),
            'vessel_types': sorted(vessel_types)
        }
    
    def analyze_training_logs(self):
        """åˆ†æè®­ç»ƒæ—¥å¿—"""
        print("\nğŸ” åˆ†æ2: è®­ç»ƒæ—¥å¿—æ£€æŸ¥")
        print("="*60)
        
        # æŸ¥æ‰¾æ—¥å¿—æ–‡ä»¶
        log_files = {
            'training_log': os.path.join(self.log_dir, 'training_log.txt'),
            'metrics': os.path.join(self.log_dir, 'metrics.txt'),
            'config': os.path.join(self.log_dir, 'config.json')
        }
        
        results = {}
        
        # æ£€æŸ¥é…ç½®æ–‡ä»¶
        if os.path.exists(log_files['config']):
            try:
                with open(log_files['config'], 'r', encoding='utf-8') as f:
                    config = json.load(f)
                print(f"ğŸ“‹ è®­ç»ƒé…ç½®:")
                print(f"  æ¨¡å‹: {config.get('model', 'Unknown')}")
                print(f"  è®­ç»ƒè½®æ•°: {config.get('epochs', 'Unknown')}")
                print(f"  å­¦ä¹ ç‡: {config.get('learning_rate', 'Unknown')}")
                print(f"  èŠ‚ç‚¹æ‰¹å¤§å°: {config.get('node_batch_size', 'Unknown')}")
                print(f"  è®¾å¤‡: {config.get('device', 'Unknown')}")
                
                enhanced_features = config.get('enhanced_features', {})
                if enhanced_features:
                    print(f"  å¢å¼ºåŠŸèƒ½: {enhanced_features}")
                
                results['config'] = config
            except Exception as e:
                print(f"âŒ æ— æ³•è¯»å–é…ç½®æ–‡ä»¶: {e}")
        
        # åˆ†æè®­ç»ƒæ—¥å¿—
        if os.path.exists(log_files['training_log']):
            try:
                with open(log_files['training_log'], 'r', encoding='utf-8') as f:
                    log_content = f.read()
                
                # æå–å…³é”®ä¿¡æ¯
                lines = log_content.split('\n')
                
                # æŸ¥æ‰¾æ•°æ®é›†ä¿¡æ¯
                print(f"\nğŸ“Š è®­ç»ƒæ•°æ®ä¿¡æ¯:")
                for line in lines:
                    if 'æ¡ˆä¾‹' in line and ('è®­ç»ƒ' in line or 'éªŒè¯' in line):
                        print(f"  {line.strip()}")
                    elif 'Using' in line and 'cases' in line:
                        print(f"  {line.strip()}")
                    elif 'Selected' in line and 'cases' in line:
                        print(f"  {line.strip()}")
                
                # æŸ¥æ‰¾è®­ç»ƒè¿›åº¦
                epoch_results = []
                print(f"\nğŸ“ˆ è®­ç»ƒè¿›åº¦ (æœ€å10ä¸ªepoch):")
                for line in lines[-200:]:  # æ£€æŸ¥æœ€å200è¡Œ
                    if 'Epoch' in line and 'Results' in line:
                        epoch_line = line.strip()
                        print(f"  {epoch_line}")
                        
                        # å°è¯•æå–æ•°å€¼
                        try:
                            if 'Training' in line and 'Accuracy' in line:
                                parts = line.split('Accuracy: ')
                                if len(parts) > 1:
                                    acc_str = parts[1].split('%')[0]
                                    acc = float(acc_str)
                                    epoch_results.append(acc)
                        except:
                            pass
                
                results['epoch_results'] = epoch_results
                results['total_lines'] = len(lines)
                
            except Exception as e:
                print(f"âŒ æ— æ³•è¯»å–è®­ç»ƒæ—¥å¿—: {e}")
        
        return results
    
    def analyze_vessel_consistency(self, data_analysis):
        """åˆ†æè¡€ç®¡ä¸€è‡´æ€§"""
        print("\nğŸ” åˆ†æ3: è¡€ç®¡ç±»å‹ä¸€è‡´æ€§æ£€æŸ¥")
        print("="*60)
        
        if 'case_info' not in data_analysis:
            print("âŒ ç¼ºå°‘æ•°æ®ä¿¡æ¯ï¼Œæ— æ³•è¿›è¡Œä¸€è‡´æ€§æ£€æŸ¥")
            return {}
        
        inconsistencies = []
        perfect_matches = 0
        total_checks = 0
        
        for case_id, info in data_analysis['case_info'].items():
            if 'vessel_types' not in info or 'node_classes' not in info:
                continue
            
            print(f"\nğŸ“ æ¡ˆä¾‹ {case_id}:")
            vessel_types = info['vessel_types']
            node_classes = info['node_classes']
            
            # æ£€æŸ¥æ¯ç§è¡€ç®¡ç±»å‹
            for vessel_type in vessel_types:
                if vessel_type in self.vessel_hierarchy:
                    expected_class = self.vessel_hierarchy[vessel_type]['expected_class']
                    
                    # åœ¨å®é™…æ•°æ®ä¸­æŸ¥æ‰¾è¿™ç§è¡€ç®¡çš„èŠ‚ç‚¹
                    vessel_nodes = []
                    # è¿™é‡Œéœ€è¦å…·ä½“çš„è¡€ç®¡èŒƒå›´ä¿¡æ¯æ‰èƒ½å‡†ç¡®æ£€æŸ¥
                    # æš‚æ—¶è·³è¿‡è¯¦ç»†æ£€æŸ¥
                    
                    total_checks += 1
                    # å‡è®¾æ£€æŸ¥ç»“æœ
                    print(f"  {vessel_type}: æœŸæœ›ç±»åˆ«{expected_class}")
        
        print(f"\nğŸ¯ ä¸€è‡´æ€§åˆ†æç»“æœ:")
        if total_checks > 0:
            print(f"  æ£€æŸ¥é¡¹ç›®: {total_checks}")
            print(f"  å®Œç¾åŒ¹é…: {perfect_matches}")
            print(f"  ä¸€è‡´æ€§æ¯”ä¾‹: {100*perfect_matches/total_checks:.1f}%")
        else:
            print("  æ— æ³•è¿›è¡Œè¯¦ç»†ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆéœ€è¦è¡€ç®¡èŒƒå›´ä¿¡æ¯ï¼‰")
        
        return {
            'total_checks': total_checks,
            'perfect_matches': perfect_matches,
            'inconsistencies': inconsistencies
        }
    
    def analyze_potential_overfitting(self, data_analysis, training_analysis):
        """åˆ†ææ½œåœ¨çš„è¿‡æ‹Ÿåˆé—®é¢˜"""
        print("\nğŸ” åˆ†æ4: è¿‡æ‹Ÿåˆé£é™©è¯„ä¼°")
        print("="*60)
        
        risk_factors = []
        risk_score = 0
        
        # 1. æ•°æ®è§„æ¨¡é£é™©
        total_cases = data_analysis['total_stats']['cases']
        total_nodes = data_analysis['total_stats']['nodes']
        
        print(f"ğŸ“Š æ•°æ®è§„æ¨¡åˆ†æ:")
        print(f"  æ€»æ¡ˆä¾‹æ•°: {total_cases}")
        print(f"  æ€»èŠ‚ç‚¹æ•°: {total_nodes:,}")
        
        if total_cases < 10:
            risk_factors.append("æ¡ˆä¾‹æ•°è¿‡å°‘(<10)")
            risk_score += 30
            print("  âš ï¸  æ¡ˆä¾‹æ•°è¿‡å°‘ï¼Œé«˜è¿‡æ‹Ÿåˆé£é™©")
        elif total_cases < 20:
            risk_factors.append("æ¡ˆä¾‹æ•°è¾ƒå°‘(<20)")
            risk_score += 15
            print("  âš ï¸  æ¡ˆä¾‹æ•°è¾ƒå°‘ï¼Œå­˜åœ¨è¿‡æ‹Ÿåˆé£é™©")
        else:
            print("  âœ… æ¡ˆä¾‹æ•°ç›¸å¯¹å……è¶³")
        
        if total_nodes < 500:
            risk_factors.append("èŠ‚ç‚¹æ•°è¿‡å°‘(<500)")
            risk_score += 20
            print("  âš ï¸  èŠ‚ç‚¹æ•°è¿‡å°‘ï¼Œå¯èƒ½å½±å“æ³›åŒ–")
        elif total_nodes < 1000:
            risk_factors.append("èŠ‚ç‚¹æ•°è¾ƒå°‘(<1000)")
            risk_score += 10
            print("  âš ï¸  èŠ‚ç‚¹æ•°è¾ƒå°‘")
        else:
            print("  âœ… èŠ‚ç‚¹æ•°å……è¶³")
        
        # 2. è®­ç»ƒå¿«é€Ÿæ”¶æ•›é£é™©
        if 'epoch_results' in training_analysis and training_analysis['epoch_results']:
            epoch_accs = training_analysis['epoch_results']
            if len(epoch_accs) > 5:
                early_acc = np.mean(epoch_accs[:5])
                late_acc = np.mean(epoch_accs[-5:])
                
                print(f"\nğŸ“ˆ è®­ç»ƒæ”¶æ•›åˆ†æ:")
                print(f"  å‰æœŸå‡†ç¡®ç‡: {early_acc:.2f}%")
                print(f"  åæœŸå‡†ç¡®ç‡: {late_acc:.2f}%")
                
                if late_acc >= 99.5:
                    risk_factors.append("å‡†ç¡®ç‡è¿‡é«˜(â‰¥99.5%)")
                    risk_score += 25
                    print("  âš ï¸  å‡†ç¡®ç‡å¼‚å¸¸é«˜ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ")
                
                if late_acc - early_acc > 30:
                    risk_factors.append("è®­ç»ƒæ”¶æ•›è¿‡å¿«")
                    risk_score += 15
                    print("  âš ï¸  è®­ç»ƒæ”¶æ•›è¿‡å¿«")
        
        # 3. ç±»åˆ«ä¸å¹³è¡¡é£é™©
        class_dist = data_analysis['class_distribution']
        if class_dist:
            max_count = max(class_dist.values())
            min_count = min(class_dist.values())
            imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')
            
            if imbalance_ratio > 20:
                risk_factors.append(f"ä¸¥é‡ç±»åˆ«ä¸å¹³è¡¡({imbalance_ratio:.1f}:1)")
                risk_score += 20
            elif imbalance_ratio > 10:
                risk_factors.append(f"ç±»åˆ«ä¸å¹³è¡¡({imbalance_ratio:.1f}:1)")
                risk_score += 10
        
        # 4. è®­ç»ƒæ—¶é—´é£é™©
        print(f"\nâ±ï¸  è®­ç»ƒæ—¶é—´åˆ†æ:")
        print(f"  æ€»è®­ç»ƒæ—¶é—´: 1.91 åˆ†é’Ÿ")
        if 1.91 < 5:  # è®­ç»ƒæ—¶é—´è¿‡çŸ­
            risk_factors.append("è®­ç»ƒæ—¶é—´è¿‡çŸ­(<5åˆ†é’Ÿ)")
            risk_score += 15
            print("  âš ï¸  è®­ç»ƒæ—¶é—´è¿‡çŸ­ï¼Œå¯èƒ½å­¦ä¹ ä¸å……åˆ†")
        
        # ç»¼åˆè¯„ä¼°
        print(f"\nğŸ¯ è¿‡æ‹Ÿåˆé£é™©è¯„ä¼°:")
        print(f"  é£é™©è¯„åˆ†: {risk_score}/100")
        print(f"  é£é™©å› ç´ : {len(risk_factors)}ä¸ª")
        
        for factor in risk_factors:
            print(f"    - {factor}")
        
        if risk_score >= 60:
            print("  ğŸš¨ é«˜è¿‡æ‹Ÿåˆé£é™©")
            conclusion = "high_risk"
        elif risk_score >= 30:
            print("  âš ï¸  ä¸­ç­‰è¿‡æ‹Ÿåˆé£é™©")
            conclusion = "medium_risk"
        else:
            print("  âœ… ä½è¿‡æ‹Ÿåˆé£é™©")
            conclusion = "low_risk"
        
        return {
            'risk_score': risk_score,
            'risk_factors': risk_factors,
            'conclusion': conclusion
        }
    
    def suggest_verification_experiments(self, overfitting_analysis):
        """å»ºè®®éªŒè¯å®éªŒ"""
        print("\nğŸ’¡ å»ºè®®éªŒè¯å®éªŒ")
        print("="*60)
        
        suggestions = []
        
        if overfitting_analysis['risk_score'] >= 30:
            print("ğŸ§ª é«˜é£é™©æƒ…å†µå»ºè®®:")
            
            suggestions.extend([
                "1. KæŠ˜äº¤å‰éªŒè¯ (K=5)",
                "2. ç•™ä¸€æ³•äº¤å‰éªŒè¯ (Leave-One-Out)",
                "3. æ—¶é—´åˆ†å‰²éªŒè¯ (æŒ‰æ—¶é—´åˆ’åˆ†è®­ç»ƒ/éªŒè¯)",
                "4. ç§»é™¤è¡€ç®¡å…ˆéªŒçº¦æŸé‡æ–°è®­ç»ƒ",
                "5. å‡å°‘æ¨¡å‹å¤æ‚åº¦å®éªŒ",
                "6. æ•°æ®å¢å¼ºå®éªŒ"
            ])
            
            for suggestion in suggestions[:4]:
                print(f"  {suggestion}")
        
        print("\nğŸ”¬ é€šç”¨å»ºè®®éªŒè¯:")
        general_suggestions = [
            "1. ç”Ÿæˆè¯¦ç»†æ··æ·†çŸ©é˜µ",
            "2. åˆ†æé”™è¯¯åˆ†ç±»æ¡ˆä¾‹",
            "3. ç‰¹å¾é‡è¦æ€§åˆ†æ",
            "4. æ¨¡å‹è§£é‡Šæ€§åˆ†æ",
            "5. åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸ŠéªŒè¯"
        ]
        
        for suggestion in general_suggestions:
            print(f"  {suggestion}")
        
        return suggestions + general_suggestions
    
    def generate_verification_commands(self):
        """ç”ŸæˆéªŒè¯å‘½ä»¤"""
        print("\nğŸš€ éªŒè¯å‘½ä»¤ç”Ÿæˆ")
        print("="*60)
        
        commands = [
            "# 1. KæŠ˜äº¤å‰éªŒè¯",
            "python create_k_fold_validation.py --k 5",
            "",
            "# 2. ç§»é™¤è¡€ç®¡å…ˆéªŒé‡æ–°è®­ç»ƒ",
            "python train.py --vessel_consistency_weight 0.0 --enable_vessel_aware False --epochs 30",
            "",
            "# 3. ç”Ÿæˆæ··æ·†çŸ©é˜µå’Œåˆ†æ",
            "python train.py --save_confusion_matrix --enable_visualization --epochs 20",
            "",
            "# 4. ç‰¹å¾é‡è¦æ€§åˆ†æ",
            "python analyze_feature_importance.py --model_path outputs/checkpoints/best.pth",
            "",
            "# 5. ç®€åŒ–æ¨¡å‹å¯¹æ¯”å®éªŒ",
            "python train_baseline.py --model simple_gnn --epochs 30"
        ]
        
        for cmd in commands:
            print(cmd)
        
        return commands
    
    def comprehensive_analysis(self):
        """ç»¼åˆåˆ†æ"""
        print("ğŸ” CPR-TaG-Netè®­ç»ƒç»“æœçœŸå®æ€§éªŒè¯")
        print("="*80)
        
        # æ‰§è¡Œæ‰€æœ‰åˆ†æ
        data_analysis = self.analyze_data_scale()
        training_analysis = self.analyze_training_logs()
        consistency_analysis = self.analyze_vessel_consistency(data_analysis)
        overfitting_analysis = self.analyze_potential_overfitting(data_analysis, training_analysis)
        
        # ç”Ÿæˆå»ºè®®
        suggestions = self.suggest_verification_experiments(overfitting_analysis)
        commands = self.generate_verification_commands()
        
        # æœ€ç»ˆç»“è®º
        print("\n" + "="*80)
        print("ğŸ¯ æœ€ç»ˆåˆ†æç»“è®º")
        print("="*80)
        
        total_cases = data_analysis['total_stats']['cases']
        total_nodes = data_analysis['total_stats']['nodes']
        risk_score = overfitting_analysis['risk_score']
        
        print(f"ğŸ“Š æ•°æ®æ¦‚è§ˆ:")
        print(f"  æ¡ˆä¾‹æ•°: {total_cases}")
        print(f"  èŠ‚ç‚¹æ•°: {total_nodes:,}")
        print(f"  è¡€ç®¡ç±»å‹: {len(data_analysis['vessel_types'])}")
        
        print(f"\nâš ï¸  é£é™©è¯„ä¼°:")
        print(f"  è¿‡æ‹Ÿåˆé£é™©è¯„åˆ†: {risk_score}/100")
        print(f"  é£é™©ç­‰çº§: {overfitting_analysis['conclusion']}")
        
        if risk_score >= 60:
            print(f"\nâŒ ç»“è®º: 100%å‡†ç¡®ç‡å¾ˆå¯èƒ½æ˜¯è¿‡æ‹Ÿåˆ")
            print(f"   ä¸»è¦åŸå› :")
            for factor in overfitting_analysis['risk_factors'][:3]:
                print(f"     - {factor}")
            print(f"   å»ºè®®ç«‹å³è¿›è¡ŒéªŒè¯å®éªŒ")
        elif risk_score >= 30:
            print(f"\nâš ï¸  ç»“è®º: 100%å‡†ç¡®ç‡éœ€è¦è¿›ä¸€æ­¥éªŒè¯")
            print(f"   å­˜åœ¨è¿‡æ‹Ÿåˆé£é™©ï¼Œå»ºè®®è¿›è¡ŒéªŒè¯å®éªŒ")
        else:
            print(f"\nâœ… ç»“è®º: 100%å‡†ç¡®ç‡å¯èƒ½æ˜¯çœŸå®æ€§èƒ½")
            print(f"   è¡€ç®¡æ„ŸçŸ¥è®­ç»ƒçš„å¼ºå…ˆéªŒçº¦æŸå¯èƒ½ç¡®å®å¤§å¤§ç®€åŒ–äº†ä»»åŠ¡")
        
        print(f"\nğŸ’¡ ä¼˜å…ˆå»ºè®®:")
        priority_suggestions = suggestions[:3]
        for i, suggestion in enumerate(priority_suggestions, 1):
            print(f"   {i}. {suggestion}")
        
        return {
            'data_analysis': data_analysis,
            'training_analysis': training_analysis,
            'consistency_analysis': consistency_analysis,
            'overfitting_analysis': overfitting_analysis,
            'suggestions': suggestions
        }

def main():
    data_dir = "/home/lihe/classify/lungmap/data/processed"
    log_dir = "/home/lihe/classify/lungmap/outputs/logs/cpr_tagnet_training_20250807_230518"
    
    if not os.path.exists(data_dir):
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return
    
    if not os.path.exists(log_dir):
        print(f"âŒ æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {log_dir}")
        return
    
    analyzer = TrainingResultAnalyzer(data_dir, log_dir)
    results = analyzer.comprehensive_analysis()
    
    # ä¿å­˜åˆ†æç»“æœ
    output_file = "training_analysis_report.json"
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            # è½¬æ¢numpyç±»å‹ä»¥ä¾¿JSONåºåˆ—åŒ–
            def convert_numpy(obj):
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                return obj
            
            # é€’å½’è½¬æ¢
            def clean_for_json(obj):
                if isinstance(obj, dict):
                    return {k: clean_for_json(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [clean_for_json(v) for v in obj]
                else:
                    return convert_numpy(obj)
            
            clean_results = clean_for_json(results)
            json.dump(clean_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“ è¯¦ç»†åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
    except Exception as e:
        print(f"âš ï¸  æ— æ³•ä¿å­˜åˆ†ææŠ¥å‘Š: {e}")

if __name__ == "__main__":
    main()
